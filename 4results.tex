\chapter{Experiments and Results}\label{results}

We performed experiments, with the previously described two variants of the proposed model. Each experiment was done with corpus 45, 462 and 90582.

\section{Input and Ouput Coding}

\paragraph{Model Variant-1:} A raw sentence is presented to the model, where each word in the sentence is processed across time by both word2vec model and ESN. The word2vec model outputs the $E_{v}=50$ dimension word embedding which is then used as input for ESN. Thus input layer have 50 neurons. For the experiment with corpus-462 we used a reservoir with 1000 neurons and the readout layer with 24 $(4 \times 3 \times 2)$ neurons as the corpus contains sentences having maximum of 4 nouns each having 3 possible roles (Agent, Object and Recipient) with respect to a maximum of 2 verbs. Output neuron have an activation 1 if the role is present in the sentence, -1 otherwise. Whereas when using corpus-90582 for training the number of neurons in reservoir were raised to 5000 and also the  readout neurons are increased to 30 $(5 \times 3 \times 2)$ as there were maximum of 5 nouns in the sentences of this corpus.

\paragraph{Model Variant-2:} In this model variant a raw sentence is presented to the model, where each word(argument) along with the verb(Predicate) with respect to which the word is currently processed is input to the model across time(see \ref{}). A sentence is processed as many time as there are verbs in the sentence. The word2vec model firstly takes this argument-predicate pair as an input and outputs a vector of $E_{v}=2 \times 50$ dimension, which is then used as an input to ESN. Thus input layer have 100 neurons where first 50 neurons encodes the vector representation of the word and remaing 50 neurons codes for the verb with respect to which word is processed. The reservoir of size 1000, 3000 is used for corpus-462 and corpus-90582 respectively. Unlike the model variant-1, the size of readout neurons always remains the same and contains 5 neurons each coding for a role: Predicate (P), Agent(A), Object(O), Recipient(R) and No Role(XX). Readout neuron of ESN have an activation 1 if the input word(argument) have the corresponding role, -1 otherwise.

\section{Experiments}

\subsection{Experiment-1: Learning thematic roles}

In order to determine the model capability for predicting thematic roles of the sentences using word2vec embeddings for words, we first did the experiment using 26 sentences(sentence 15 to 40, in corpus-45) from corpus-45. The chosen sentences have distinct surface form (e.g. active, passive, dative-passive) and grammatical structure. This also include the sentences with single verb or double verb relative surface form[ref?]. Both the model variants(see section-?) learned the sentences without any error when trained and tested on all the sentences. To test the performance and generalization capabilities of model on untrained sentences, we performed a leave-one-out cross validation, where a model is trained on 25 sentences out of 26 and tested on remaining 1 sentence.

\paragraph{Model Variant-1:} The model variant-1 with a reservoir of size 1000 units the model yielded [….] meaning error and [….] sentence error in sentence continuous learning mode and [….] meaning error and […..] sentence error in Sentence final learning mode. The results were averaged over 10 reservoir instances. For Continuous learning model the spectral radius(SR), input scaling (IS) and leak rate($\alpha$) were identified as $SR=[?], IS=[?], \alpha=[?]$.` Whereas for Sentence final learning mode the $SR=?, IS=?, \alpha=?$. The ESN parameters for which the optimized results are identified are found by exploration of parameter space. As one may note that difference in training and test error for both the metrics(meaning and sentence) is large. This indicates the model is overfitting on the dataset. However, it is not surprisring because the dataset contains limited examples, which constrained model to genralize well. As this experiments remains a toy demonstration we will explore the generalization capabilty of the model in section \ref{exp-2} .

\paragraph{Model Variant-2:} The model Variant-2, on the other hand, with a reservoir of size 600 neurons, produced the classification scores [write score here] during cross-validation. 

\subsection{Experiment-2: Generalization Capabilities} \label{exp-2}

In the previous experiment we demonstrated the performance of the model with the limited set of sentences where the results suggested that the model is probably overfitting and not generalizing on unseen data. So in order to test the generalization capability of the model, we examine the model behavious with an extended corpus with 462 sentences(see section corpus-462) using 10-fold cross validation. Corpus-462 with 462 sentences was randomly split into 10 equally sized subsets(i.e. each subset with ~46 sentences). The model was trained on sentences from 9 subsets and then tested on remaining one subset. This process was repeated 10 times such that the model is trained and tested on all the subsets atleast once. 

\paragraph{Model Variant-1:} We initially trained and tested the model on all the 462 sentences. The model learned the full corpus-462 with $0.54\%$ meaning error and $1.51\%$ sentence error in continous learning mode and $0.07\%$ meaning error and $0.21\%$ sentence error in final learning mode. Using the 10-fold cross validation the model generalized to $7.66\% (\pm 1.62\%) $ meaning error and $20.26\% (\pm 2.76\%)$ sentence error in continous learning mode with sepctral radius(SR), input scaling(IS) and leak rate(LR) of $2.4$, $2.5$ and $0.07$ respectively. Whereas in the sentence final learning mode with $SR=2.2$, $IS=2.3$ and $LR=0.13$, the optimal meaning and sentence error were observed as $8.79\% (\pm 1.24\%)$ and $23.69\% (\pm 1.52\%)$ respectively. The optimal parameters were identified using grid search over the parameter space. 

	Using word2vec vector embeddings of words as compared to localist representation we clearly see an improvement in meaning error from $... \%$ to $... \%$ and sentence error from $... \%$ to $... \%$ in SCL mode. In SFL model the meaning error reduced from  $...\%$ to $...\%$ and sentence error from $...\%$ to $...\%$.
	
As illustrated in Table \ref{tab:corpus-structure}, we observed that there is an %explaination goes here%

\paragraph{Model Variant-2:} The model variant-2 with a reservoir of size 1000 neurons when trained and tested on all 462 sentences of corpus-462, learned to label the roles of words in the sentences with an accuracy(A),precision(P), recall(R) and F1-Score(F1) of $96.91\%$, $96.98\%$, $96.91\%$, $96.74\%$ respectively. When the model varaint tested using 10-fold cross validation we got A=$97.18\% (\pm 0.11\%)$, P= $96.86\%(\pm 0.49\%)$, R=$91.93\%(\pm 0.23\%)$ and F1=$94.16\%(\pm 0.26\%)$ with $IS=1.15, SR=0.7, LR=0.1$. The marginal difference between the training and cross validation scores indicates that the model variant is generalizing well even on untrained data and is not overfitting. The precision, recall and f1-score for individual role is listed in table \ref{tab:classsification-scores}.

% modify evaluation metrics section and change to macro from weighted average
%analyse table and confusion matrix

\begin{table}
\centering
\begin{threeparttable}
\caption{Training and testing classification scores for individual roles}
\label{tab:classsification-scores}
\rowcolors{2}{gray!25}{white}
\begin{tabular}{llcccc}
\toprule
Role & & \( Precision \) & \( Recall \) & \( F1 \ Score \) & \( Support \) \\
\midrule                 
\textbf{Agent}&test & 0.92 & 0.79 & 0.85 & 888 \\
			& train & 0.94 & 0.80 & 0.88 & 892 \\
\textbf{Object}&test & 0.95 & 0.81 & 0.88 & 791 \\
			& train & 0.96 & 0.81 & 0.88 & 794 \\
\textbf{Recipient}&test & 1.00 & 1.00 & 1.00 & 383 \\
			& train & 1.00 & 1.00 & 1.00 & 384 \\
\textbf{Verb}&test & 1.00 & 1.00 & 1.00 & 888 \\
			& train & 1.00 & 1.00 & 1.00 & 892 \\
\textbf{No Role}& test & 0.97 & 1.00 & 0.99 & 9785 \\
			& train & 0.97 & 1.00 & 0.99 & 9823 \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\small
\item Comparision of training and cross validation scores for each output roles predicted by the model. Support for each role: actual number of instance, is also shown in last column. Simulation were done using 1000 reservoir neurons with SR=0.7, ISS=1.15, LR=0.1.
\end{tablenotes}
\end{threeparttable}
\end{table}

\begin{figure}[hbtp]
\centering
\includegraphics[width=1.0\linewidth]{confusion_matrix}
\caption{\textbf{Normalized Confusion matrix:} Description goes here.}
\label{fig:confusion_matrix}
\end{figure}

In order to compare the performance of model variant-2 while using word2vec embeddings for words over localist vector representation, we also performed a 10-fold cross validation using localist vector representation of the words for the input sentences. The observed results indicates that the model variant-2 performs better with word2vec vectors of words than localist vector representation. 

\subsection{Experiment-3: Effect of Corpus structure} 

Recall that the corpus-462 was created based on context free grammer. Thus the sentences in the corpus contains inherent grammatical structure. The model is thus possibly utilizing the underlying grammatical structure to some extend for learning and generalizes well. To test this hypothesis and to demonstrate that the model is not generalizing on any inconsitent regularity in the corpus, we removed the inherent grammatical structure from the sentence in the corpus by randomizing the word orders within the sentences. Such a test will also help us to have insight on what the model is actually learning and whether the model is overfitting or not. The situation of overfitting typically occurs when the corpus size is significantally less than the number of trainable parameters[ref?]. In the model variant-1, having reservoir of size 1000 neurons and 42 readout neurons the number of trainable parameters are 42000 $(42\times 1000)$  whereas the model variant-2 with reservoir size $600$ and $5$ readout neuron the trainable parameters are 3000 $(5 \times 600)$. In both the case the number of trainable parameters are significantally greater than our corpus size (462 sentence). This is thus a possible situation of overfitting.

We presented the corpus with scrambled sentences(i.e. in absence of any grammatical structure) to both the model variants and performed a 10 fold cross-validation. The cross validation errors obtained in the previous experiment on the corpus with grammatical structure can then be compared with the cross validation error obtained while using scrambled corpus. If the model is not overfitting and learning from the grammatical structure then the model should generalize better for the corpus with unscrambled(i.e. in presence of grammatical structure) sentences. However in case of overfitting the generalization effect should not vary much both in presence and absence of grammatical structure in the corpus.

As illustrated in Table.[no], we observed that ....
\begin{table}
\centering
\begin{threeparttable}
\caption{Mean and standard deviation of meaning and sentence error on train and test set of coprus-462 in different learning modes.}
\label{tab:corpus-structure}
\rowcolors{2}{white}{gray!25}
\begin{tabular}{llllllllll}
  \toprule
  &  & \multicolumn{4}{c}{Our Approach} & \multicolumn{4}{c}{Xavier's Approach} \\
  \cmidrule(lr){3-6} \cmidrule(lr){7-10}
  
  \multirow{3}{*}{\raisebox{-\heavyrulewidth}{}} & \multirow{3}{*}{\raisebox{-\heavyrulewidth}{}} & \multicolumn{2}{l}{Corpus-462} & \multicolumn{2}{l}{462 Scrambled} & \multicolumn{2}{l}{Corpus-462} & \multicolumn{2}{l}{462 Scrambled} \\
  \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10}
  
  						& 		& me & se & me & se	& me & se & me & se \\
  \midrule
  \textbf{SCL\ train} 	& mean 	& 0.541 & 1.515 & 1 & 2 	& 0.123 & 1.207 & 4.813 & 20.433  \\
   			    		& std 	& 0.000 & 0.000 & 1 & 2 	& 0.029 & 0.297 & 0.299 & 1.251  \\
  \textbf{SCL\ test} 	& mean  & 7.663 & 20.260 & 69.761 & 99.130 	& 7.433 & 32.130 & 74.154 & 99.891 \\
  			   			& std  	& 1.629 & 2.761 & 1.462 & 1.064  	& 0.524 & 1.353 & 0.802 & 0.146\\
  \textbf{SFL\ train} 	& mean 	& 0.072 & 0.216 & 8.531 & 25.757 	& 0.000 & 0.000 & 0.000 & 0.000  \\
  				 		& std 	& 0.000 & 0.000 & 0.000 & 0.000 	& 0.000 & 0.000 & 0.000 & 0.000  \\
  \textbf{SFL\ test}	& mean  & 8.780 & 23.695 & 69.275 & 99.347 	& 9.178 & 24.370 & 73.391 & 99.913 \\
  			  			& std 	& 1.245 & 1.486 & 2.143 & 0.996  	& 0.574 & 1.192 & 0.962 & 0.106  \\
  \bottomrule
\end{tabular}
\begin{tablenotes}
\small
\item 
Meaning (me) and Sentence error (se) in different learning modes with our approach of using word2vec embeddings and Xavier's \cite{xavier:2013:RT} approach of using grammatical construction and localist representation of words. The errors are given in percentage. For Sentence Final Learning mode (SFL): our approach (ISS=, SR= ,LR= ) and Xavier's approach(ISS=,SR=,LR=). For Sentence Continuous Learning mode (SCL): our approach (IS=2.5, SR=2.4, LR=0.07) and Xavier's approach(ISS=,SR=,LR=). Simulation were done with 10 reservoir instance of 1000 neurons.
\end{tablenotes}
\end{threeparttable}
\end{table}

\subsection{Experiment-4: Effect of Reservoir size}

One of the important hyperparmeter which effects the performance of the model is the size of reservoir (i.e. number of neurons in the reservoir). Also the addition of neurons in the reserovir is computationally inexpensive, because the read-out weights ($W^{out}$) scales linearly with the number of neurons in the reservoir[ref?]. So, in order to determine the impact of reservoir size on the peformance of the model variant-1, we plotted the cross validation errors against number of neurons in the reservoir.

\begin{figure}[hbtp]
\centering
\includegraphics[width=1.0\linewidth]{reservoir_size_1}
\caption{\textbf{Effect of reservoir size on cross validation errors on Model Varinat-1:} Description goes here.}
\label{fig:sem_rel}
\end{figure}

\begin{figure}[hbtp]
\centering
\includegraphics[width=1.0\linewidth]{reservoir_size_1}
\caption{\textbf{Effect of reservoir size on cross validation errors on Model Variant-2:} Description goes here.}
\label{fig:sem_rel}
\end{figure}

%TODO: Write the effect of reservoir size on the model variant-2

\subsection{Experiment-5: Effect of Corpus size}

In the previous experiments we noticed that the errors rates for model variant-1 and classification scores for model variant [] improved as we extended the corpus size from 45 to 462 sentences. To investigate the effect of corpus size and scaling capability of the model, we used extended corpus-90582(see section) for this experiment. As the corpus also contains $12\%$ of ambiguous sentences which impede the learning and generalization if the model, this experimtent will also validate the model ability to process the abmbiguous sentences.

In order to study the generalization capabilty of the model with different corpus size, we created 6 sub-corpora each containing $6\%$, $12\%$, $25\%$, $50\%$, $75\%$, $100\%$ of sentences by randomly sampling from the orginal corpora with 90582 sentences[ref?]. Each of the sub-corpora is exposed to the model and 2-fold cross validation is performed to evaluate the model performance. In 2-fold cross validation the model is trained on half the subcorpora size and tested on remaining half. The second half used for testing is then used to train the model and then tested on the first half used for training previously. Figure [ref] and Figure [ref] shows the cross validation errors rates with respect to corpus size for model variant-1 and variant-2 respectively.

\begin{figure}[hbtp]
\centering
\includegraphics[width=1.0\linewidth]{corpus_size_1}
\caption{\textbf{Effect of corpus size on cross validation errors:} Description goes here.}[ESN9]
\label{fig:sem_rel}
\end{figure}

\subsection{Experiment-6: Online reanalysis of sentences}

 
 

 
 


