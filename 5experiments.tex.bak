\chapter{Experiments}\label{experiments}

This chapter presents the experimental setup and experiments performed in this research work. The rest of the chapter is organized as follows. First, we will have an overview of the corpora used to carry out the experiments. Then we describe the configuration of the model necessary to perform the TRA task with the Word2Vec-$\theta$RARes model and the Word2Vec-ESN classifier proposed in the previous chapter. We will also see the experimental setup to perform the TRA task.

\section{Corpora and pre-processing}\label{corpora}

This section describes the corpora used to perform experiments with the proposed Word2Vec-$\theta$RARes language model and Word2Vec-ESN classifier for TRA task. Firstly, we will have an overview of the corpora used for TRA task and then we will see the corpus used to train Word2Vec model.

\subsection{Corpora For TRA Task}

In order to have a fair comparison between the Word2Vec-$\theta$RARes model and  $\theta$RARes model on the TRA task, the same corpora \footnote{https://sites.google.com/site/xavierhinaut/downloads} used by Hinaut et al. \cite{xavier:2013:RT, tra:xavier_hri} with  $\theta$RARes model was utilized. Thus, the corpus-373, corpus-462, corpus-90582 containing 373, 462 and 90582 sentences respectively, were used to perform the experiments with the proposed model on TRA task. 

\paragraph{Corpus-45:} Corpus-45 is a small corpus of 45 sentences. This corpus contains the constructions in grammatical forms i.e. `N' and `V' tokens were used to represent the nouns and verbs, along with the coded meaning of each sentence. The corpus contains the constructions with different grammatical structures i.e. active, passive, object-relative, subject-relative sentences which are represented in the form:

\begin{enumerate}[noitemsep]
\item the N V the N . \# N1-A1 N2-O1
\item the N was V by the N . \# N1-O1 N2-A1
\end{enumerate}

The coded meaning of the sentences is represented after the `$\#$' token. The coded meaning `N1-A1' can be interpreted as; the first noun is the agent of the first verb. For the object and the recipient roles `O' and `R' tokens were used respectively.

\paragraph{Corpus-462 and Corpus-90582:} The sentences in the corpus-462 and corpus-90582 were generated by Hinaut et al. using a context-free grammar for English language and used for TRA task \cite{xavier:2013:RT}. Each sentence in these corpora have verbs which take one, two, or three clause elements. For example, the sentences, ``The man \textit{jumps}", ``The boy \textit{cuts} an Apple", ``John \textit{gave} the ball to Marie", have verbs with the 1 (agent), 2 (agent and object) and 3 (agent, object and recipient) clause elements respectively. The sentences in the corpora have a maximum of four nouns and two verbs \cite{xavier:2013:RT}. A maximum of one relative clause is present in the sentences; the verb in the relative clauses could take 1 or 2 clause elements (i.e., without recipient). For example, ``The dog that bit the cat chased the boy". Both the corpus-462 and corpus-90582 have the constructions in the form:

\begin{enumerate}[noitemsep]
\item walk giraffe $<\!o\!>$ AP $<\!/o\!>$ ; the giraffe walk -s . \# ['the', 'X', 'X', '-s', '.']
\item cut beaver fish , kiss fish girl $<\!o\!>$ APO , APO $<\!/o\!>$ ; the beaver cut -s the fish that kiss -s the girl . \# ['the', 'X', 'X', '-s', 'the', 'X', 'that', 'X', '-s', 'the', 'X', '.']
\end{enumerate}

Each construction in the corpus was divided into four parts. The first part describes the meaning of the sentence using the semantic words (or open class words) in order of predicate, agent, object and recipient. The second part (between `$<\!o\!>$' and `$<\!/o\!>$') describes the order of the thematic roles of semantic words as they appear in the raw sentence. The third part (between `;' and `\#') contains the raw sentence with verb inflections (i.e. `-s') and the fourth part is the abstract representation of a sentence with semantic words removed and replace with token `X' \cite{xavier:2013:RT}.

Corpus-90582 have 90582 sentences along with the coded meaning of each sentence. This corpus is redundant; multiple sentences with different grammatical structure but the same coded meaning (see fig. \ref{fig:meaning_realtions}). In total, there were only 2043 distinct coded meanings \cite{xavier:2013:RT}. This corpus also has an additional property that along with complete coded meanings of the sentences, it also have incomplete meanings. For example, the sentence ``The Ball was given to the Man‚Äù have no \textit{`Agent'}, and thus the meaning of the sentence is ``given(-, ball, man)". The corpus also contains $5268$ pair and $184$ triplets of ambiguous sentences, i.e. 10536 and 553 sentences respectively. Thus in total there were $12.24 \%$ (i.e., $ 5268 \times 2 + 184 \times 3 = 11088 $) of ambiguous sentences which have the similar grammatical structure but different coded meaning \cite{xavier:2013:RT}.

\paragraph{Corpus-373:} Apart from the corpus-462 and corpus-90582 which were artificially constructed using the context-free grammar, the corpus-373 containing real sentences collected from humans in natural language was also used. Corpus-373 includes the instructions collected from the participants interacting with a humanoid robot (iCub) in a Human-Robot Interaction (HRI) study of language acquisition conducted by Hinaut et al. \cite{tra:xavier_hri}. In the study, the robot first performs one or two actions by placing the available objects to a location (e.g. left, right, middle) and the participants observes the actions. Then the participants were asked to instruct the robot again to perform the same actions in the natural language. Thus the corpus contains 373 complex instructions to perform single or double actions with temporal correlation (see Action Performing task in Hinaut et al. \cite{tra:tra:xavier_hri} for more details). For example, the instruction ``Point to the guitar" is a one action command whereas the instruction ``Before you put the guitar on the left put the trumpet on the right" is a complex command with double actions, where the second action is specified before the first action. A list of 86 closed class words used to filter semantic words from the sentences is also provided with this corpus. Also, unlike corpus-462 and corpus-90582 this corpus does not contain verbs inflections. Thus, this data is complex enough to test the learnability and generalization of the proposed models on TRA task.

\paragraph{Pre-processing:} As described earlier, unlike the  $\theta$RARes model, the proposed models process the raw sentences. Thus the sentences in corpus-45 are manually pre-processed by replacing the token `N' and `V' with appropriate nouns and verbs such that the coded meaning of the sentence is not changed. For example, the construction ``the N V the N" was changed to ``the man pushed the ball". Recall that the corpus-462 and corpus-90582 contains the sentences where the verbs are represented along with inflections (suffixes ``-s", ``-ed", ``-ing"). So these constructions in corpus-462 and corpus-90582 were processed to obtain the raw sentences without verb inflections. Firstly, all the words are lowercased and then the verbs with inflections are replaced by conjugated verbs \footnote{service used to find verb conjugations: \text{http://www.scientificpsychic.com/cgi-bin/verbconj2.pl}}. The verb conjugation to be used depends on the inflection used for the verb. For example, the sentences ``The giraffe walk -s" and ``John eat -ed the apple" has been changed to ``The giraffe walks" and ``John ate the apple" respectively. This preprocessing was done because the distributed word representation captured by Word2Vec model already captures these syntactic relations which was imposed previously using verb inflections e.g $vector('walks') - vector('walk') \approx vector('talks') - vector('talk')$. 
 
\subsection{Corpus For Training Word2Vec Model}

In this work, to train the Word2Vec model, a general purpose dataset i.e. Wikipedia corpus \footnote{https://dumps.wikimedia.org/enwiki/latest/} ($\approx$ 14 GB) was used to obtain the low dimensional distributed embeddings of words. The corpus contains 2,65,8629 unique words. The reason for choosing the Wikipedia dataset to train the model is that it contains most of the words used in the English language. Also, the Word2Vec model does not give good quality vector representation for words when trained over a small corpus thus a general purpose data set with billions of words is required to have better word embeddings. Thus more words we have the better the vector representation of words. Once the vector representation of words in the Wikipedia dataset is obtained, the model can then be trained further on our domain specific dataset (corpus-462 and corpus-90582) with more bias toward domain specific dataset to update the previously learned vector representations. 

\section{Models Configuration for TRA task}

In this section, we will see the necessary configuration required for the proposed model. Both the Word2Vec-$\theta$RARes and Word2Vec-ESN classifier are initialized in the same way as described in the following subsections.

\subsection{Obtaining Word Embeddings} \label{get_word_embeddings}

Before using the proposed models for TRA task, the distributed embeddings of the words are required to be learned by Word2Vec model. To get the vector representation of words, the Word2Vec model is first trained on the Wikipedia dataset using skip-gram negative sampling approach (see Chapter \ref{basics} for more details). Skip-gram negative sampling approach was used because it was claimed to accelerate the training of the model and the resultant word vectors performs better on word analogy task\cite{w2v:mikolov_2013_efficient, w2v:mikolov_2013_distributed}. 

To obtain the word vectors of different dimensions, six different Word2Vec models were trained. The hidden layers of each model contain 20, 30, 50, 100, 200, 300 neurons respectively. Thus each model produces the distributed word vectors corresponding to the size of the hidden layer. All the six models were trained using the same hypermeters as follows. A context window of $\pm 5$ for the current input word was used. The negative sampling size of 5 was chosen i.e. 5 noise words are chosen randomly from the vocabulary which does not appear in the context of the current input word. Additionally, the most frequent words are discarded with the probability using equation \ref{eqn:subsampling} with a subsampling threshold of $t = 10^{-5}$. All the words which appear less than $5$ times in the corpus were also ignored. To update the network weights stochastic gradient descent was used \cite{w2v:parameter_learning, w2v:mikolov_2013_distributed} with initial learning rate of $\alpha = 0.025$, which drops to $min_alpha = 0.0001$ linearly as training progress. 

The word embeddings obtained by training the Word2Vec model on Wikipedia dataset are accurate enough to capture the semantic relationship of the words for e.g. $vec(`Paris') -vec(`France') + vec(`Germany') \approx vec(`Berlin')$. Before training the model on Wikipedia corpus, a vocabulary of words in the corpus is created. Once the vocabulary is created, and the model is trained, it was not possible to add new words to this vocabulary. However, there is a possibility that when a domain specific corpus (e.g. corpus-462, corpus-373, etc.) is used to further train the Word2Vec model, some of the words may not be present in previously generated vocabulary. Due to this limitation, it was not possible to get the distributed embeddings of these new words. Thus we needed to update the vocabulary of the model if some words are not already present in the vocabulary to facilitate the online training of Word2Vec model. Unfortunately, neither the C++ API \footnote{https://code.google.com/archive/p/word2vec/} nor the Gensim python API \cite{w2v:gensim_api} implementation of Word2Vec model supports the vocabulary update, once created. So, we implemented the online training \footnote{The code is adapted from- http://rutumulkar.com/blog/2015/word2vec} of Word2Vec by modifying and extending the Gensim API. The new words that were not present in the existing vocabulary can now be added and initialized with some random weights, and the model can then be trained in the usual manner to learn the distributed embeddings of new words. Although now the vocabulary can be updated in an online manner, the vector embeddings of the newly added words have poor quality if their count in the new corpus is less. Thus, to have more impact of the domain specific corpus on the words embeddings and to improve the quality of words vectors, the domain specific dataset was repeated several times before training the model \footnote{Idea discussed on: https://groups.google.com/forum/$\#$!topic/gensim/Z9fr0B88X0w}.

So now when we have an online version of training the Word2Vec model, the training of Word2Vec model can be resumed on the domain-specific corpora (corpus-462 and corpus-90582). While updating the model on the new dataset, no word was disregarded irrespective of its count in the corpora. This allows us to have the vector embeddings of all the words in our domain specific corpora. Once the Word2Vec model is trained, the generated word embeddings were normalized using L-2 norm before using them for TRA task. One important point to remember while training word2vec model is that if an already trained model has to be trained further on any other corpus, then normalization should not be done, as it is not possible to train the model again with normalized word vectors \cite{w2v:gensim_api}. The trained Word2Vec model is now ready to be used with the Word2Vec-$\theta$RARes model and Word2Vec-ESN classifier.

\subsection{ESN reservoir initialization}

The size of the reservoir is one of the critical parameters of ESN and it is often recommended to use a bigger reservoir that can be computationally afforded, provided the appropriate regularization method is used \cite{esn:practical_guide}. Thus for the proposed models, a reservoir of 1000 (until and unless specified) leaky integrator neurons with \textit{tanh} activation function was used. The input-to-hidden ($W^{in}$), hidden-to-hidden ($W^{res}$) weights were randomly and sparsely generated using a normal distribution with mean 0 and variance 1. For the TRA task, output-to-reservoir weights ($W^{back}$) was not used. The reservoir state update equations \ref{eqn:res_update} and \ref{eqn:res_state} thus changes to:

\begin{equation} \label{eqn:res_new_update}
x'(n) =\textit {tanh } ( W^{res}x(n-1) + W^{in}.u(n))
\end{equation}
\begin{equation} \label{eqn:res_new_state}
x(n) =\textit (1-\alpha) x(n-1) + \alpha x'(n)
\end{equation}

To generate the sparse weights a fixed fanout number of $F_{hh} = 10$ and $F_{ih} = 2$ was used i.e. each reservoir neuron was randomly connected to $10$ other reservoir neurons and each input neuron was connected to only $2$ other reservoir neurons respectively. Use of the fixed fanout number scales the computational cost of reservoir state update linearly with increase in reservoir size \cite{esn:practical_guide}. After the weights initialization, there are several other reservoir parameters which are to be optimized and are task specific. In the next subsection, we will see how these parameters are optimized.

\subsection{Learning ESN parameters} \label{grid_search}

Echo state network have several parameters to be optimized for the proposed models to perform efficiently on the TRA task. Some of the parameters like reservoir size, sparsity, distribution of non-zero weights are straightforward \cite{esn:practical_guide}. Whereas other parameters like Spectral Radius (SR), Input Scaling (IS) and Leak Rate (LR) are task dependent and are often tuned by multiple trials. Thus to identify these parameters for TRA task, a grid search over the parameter space using five reservoir instances was performed. As the parameter search space can be gigantic, a broader grid with wider parameter ranges was first explored to find the optimal grid: grid with low sentence error for Word2Vec-$\theta$RARes model and high F1-Score for the Word2Vec-ESN classifier. The optimal region identified during the broader grid search was then used for the narrow search to identify the optimal parameters \cite{esn:practical_guide}. As both the proposed models process the sentences differently and have different training objectives, the ESN parameters for both the models were optimized separately. Additionally, the Word2Vec-$\theta$RARes model parameters are optimized separately for SCL and SFL mode. In this section, we will see the parameter optimization on corpus-462, as most of the experiments were performed using this corpus. 

\paragraph{Word2Vec-$\theta$RARes model:} To optimize the reservoir parameters, corpus-462 with the topologically modified coded meaning was used. The choice of using the modified coded meaning was deliberate and will be made clear later in Experiment-6. To get the optimal parameters (i.e. parameters resulting in the lowest sentence error) of Word2Vec-$\theta$RARes model, the model was trained and tested over a range of parameters using 10-fold cross-validation method. The corpus-462 with 462 sentences was randomly split into ten equally sized subsets (i.e. each subset with $\approx$ 46 sentences). The model was trained on sentences from 9 subsets and then tested on remaining one subset. This process was repeated ten times such that the model was trained and tested on all the subsets at least once. A reservoir of 1000 neurons and a fixed regularization coefficient, $\beta = 1\mathrm{e}{-6}$ for ridge regression was used. By exploring the parameter space we identified the optimal parameters as $SR = 2.4$, $IS = 2.5$ and $LR = 0.07$ in SCL mode and $SR = 2.2$, $IS = 2.3$ and $LR = 0.13$ in SFL mode. 

\paragraph{Word2Vec-ESN classifier: } For the Word2Vec-ESN classifier, the optimal parameters (i.e. parameters resulting to highest F1-Score) were identified during the grid search using the same corpus-462 and by applying 10-fold cross-validation. Keeping all the other conditions i.e. reservoir size and regularization coefficient, identical to Word2Vec-$\theta$RARes model described above, optimal parameters were identified as $SR = 0.7$ , $IS = 1.15$, $LR = 0.1$. As later in the Experiment-2, the performance of the Word2Vec-ESN classifier will also be tested on the sentences transformed to grammatical form (i.e. semantic words replaced with 'SW' token) and represented in localist fashion. Thus it is also required to find the optimal parameters for this transformed corpus. Keeping all the conditions same, the grid search was performed over parameter space and identified the optimal parameters were identified as $SR = 1.3$, $IS = 1.0$ , $LR = 0.4$. 

\subsection{Input and Output Coding}

As specified in chapter \ref{approach}, the Word2Vec-$\theta$RARes model and Word2Vec-ESN classifier process the sentences differently, thus the input and output coding for both the model also differs. However, the initialization of Word2Vec model and ESN reservoir weights remains same for both the models. 

\paragraph{Word2Vec-$\theta$RARes model:}

A raw sentence is presented to the model, where each word in the sentence is processed across time by both Word2Vec model and ESN. The Word2Vec model outputs the $E_{v} = 50$ dimension word embedding which is then used as an input for ESN. Thus input layer of ESN has $50$ neurons.

For all the experiments on corpus 45, 462 and 90582 (Experiments 1-5 in next section), an equivalent but topologically modified coded meaning (see section \ref{sec:model_variant}) was used. The readout layer of ESN contains 24 $(4 \times 3 \times 2)$ neurons as the corpus contains sentences, having a maximum of 4 nouns each having 3 possible roles (Agent, Object, and Recipient) with respect to a maximum of 2 verbs. Each neuron in the readout layer thus codes for a role of a noun with respect to a verb. The corpus-90582 contains a maximum of 5 nouns, and thus the size of readout layer is 30 $(5 \times 3 \times 2)$. 

For Experiment-6, the topologically modified coding was not used. So, the readout layer contains 36 $(6 \times 3 \times 2)$ neurons as there are maximum of 6 semantic words in this corpus and each could take one of the 3 possible thematic roles (Object, Predicate, Location) with respect to a maximum of two actions \cite{tra:xavier_hri}. Output neurons have an activation 1 if the corresponding roles are present in the sentence, -1 otherwise. 

\paragraph{Word2Vec-ESN classifier:}

Recall that in Word2Vec-ESN classifier a raw sentence is presented to the model, where each word (argument) along with the verb (Predicate) with respect to which the word is currently processed, is input to the model across time (see section \ref{sec:model_variant}). Also, A sentence is processed as many time as there are verbs in the sentence. So, the Word2Vec model takes this argument-predicate pair as an input and outputs a vector of $E_{v} = 2 \times 50$ dimension, which is then used as an input to ESN. Thus the input layer contains $100$ neurons where first $50$ neurons encodes the vector representation of the word and remaining $50$ neurons codes for the verb with respect to which word is being processed. Unlike the Word2Vec-$\theta$RARes, the size of readout layer always remains the same and contains five neurons each coding for a role: Predicate (P), Agent (A), Object(O), Recipient (R) and No Role (XX) for both corpus-462 and corpus-90582. Readout neuron of ESN has an activation 1, if the input word-verb (argument-predicate) pair have the corresponding role, -1 otherwise.

\section{Experimental Setup}

In this section we descibe the experiments performed in this thesis. Untill and unless specified we use all the optimal parameters identified during the grid search for both Word2Vec-$\theta$RARes model and its Word2Vec-ESN classifier, described in section \ref{grid_search}. 

\subsection{Experiment-1: Models performance on small corpous}\label{exp-1}

In order to see the performance of Word2Vec-$\theta$RARes model and its Word2Vec-ESN classifier on TRA task with the small corpus, the model was first tested with the limited number of sentences i.e. 26 sentences (sentence 15 to 40) from corpus-45. The chosen sentences have distinct surface form and grammatical structure (e.g. active, passive, dative-passive). Both the models were first trained and tested on all the sentences to get the training errors and then tested using Leave one Out (LoO) cross-validation method: training on 25 sentences and testing on remaining one sentence such that all the sentences are tested at least once. Reservoir parameters for this experiments were optimized on the 26 constructions used in the experiment by exploring the parameter space and using LoO cross-validation approach. This experiment remains a toy demonstration, and we will later see the generalization capability of the model with an extended corpus in Experiment-2 and Experiment-5.
 
\paragraph{Word2Vec-$\theta$RARes model:} For simulation with Word2Vec-$\theta$RARes model, five reservoir instances each with 1000 neurons and reservoir parameters $SR = 2.1$ , $IS = 6.6$, $LR = 0.5$ was used. The model was operated in SCL mode.

\paragraph{Word2Vec-ESN classifier:} For simulations with the Word2Vec-ESN classifier, five reservoir instances each with 500 neurons and reservoir parameters $SR = 0.2 $ , $IS = 0.7$ , $ LR = 0.7$, was used. 

\subsection{Experiment-2: Generalization Capabilities} \label{exp-2}

In the experiment \ref{exp-1}, we performed simulations with the limited set of sentences. Thus in order to test the generalization capability of the model, an extended corpus of 462 sentences was examined (see corpus-462 in \ref{datasets}). 

\paragraph{Word2Vec-$\theta$RARes model:} Using the extended corpus-462, generalization capability of Word2Vec-$\theta$RARes model was tested using 10-fold cross-validation approach. For this experiment, equivalent but topologically modified output coding of sentences was used for training (see fig. \ref{fig:model_variant_2}). In order to compare the generalization ability of the Word2Vec-$\theta$RARes model and  $\theta$RARes model, simulations were performed in both the learning modes i.e. in SCL and SFL mode. The reservoir parameters identified during grid search for both the learning modes were used i.e. $SR = 2.4$, $IS = 2.5$, and $LR = 0.07$ in SCL mode and $SR = 2.2$, $IS = 2.3$ and $LR = 0.13$ SFL mode (see section \ref{grid_search}). For simulations in each learning mode, five reservoir instances (i.e. reservoir weights were initialized using 5 different random generator seeds) each with a reservoir of size 1000 neurons was used.

\paragraph{Word2Vec-ESN classifier:} Recall that the Word2Vec-ESN classifier unlike Word2Vec-$\theta$RARes model evaluate the performance of model using classification metrics(see section \ref{sec:model_variant} in chapter \ref{approach}). The model process the raw sentences and the distributed vector representation of input argument-predicate pair is fed into ESN. However, it is also important to test the behavior of this model on the sentences transformed to grammatical form and using localist word representation i.e. without the word2vec unit in the model, as an input to ESN. This will allow us to have an insight on the benefits of distributed word vector. Thus for this experiment, the Word2Vec-ESN classifier was used in two configurations mentioned below. Simulations in both the configurations were performed using five reservoir instances each with a reservoir of 1000 neurons.

\begin{enumerate}
\setlength{\itemsep}{\smallskipamount}

\item \textbf{Configuration-1: } In this configuration, the Word2Vec-ESN classifier process the raw sentences and distributed vector representation of argument-predicate pairs are input to ESN (see section \ref{sec:model_variant} for more detail). For the simulation in this configuration, the reservoir parameters $SR = 0.7$ , $IS = 1.15$, $LR = 0.1$ were used. The parameters are identified previously by exploring the parameter space. \label{config-1}

\item \textbf{Configuration-2: } In this configuration, the Word2Vec-ESN classifier is used without Word2Vec unit, and only ESN is used for processing the sentences. The input sentences were transformed in the grammatical form: all the semantic words in the sentence were replaced with 'SW' token, and localist word vectors are input to ESN. For the simulation in this configuration, the reservoir parameters $SR = 1.3$, $IS = 1.0$ , $LR = 0.4$ found previously during grid search were used. \label{config-2}

\end{enumerate} 

\subsection{Experiment-3: Effect of Corpus structure} \label{exp-3}

As described earlier, that the sentences in the corpus-462 were created based on context-free grammar. Thus the sentences in the corpus contain an inherent grammatical structure. The models thus possibly utilize the underlying grammatical structure to some extent for learning and generalizing. To test this hypothesis and to demonstrate that the model is not generalizing on any other inconsistent regularity in the corpus, in this experiment, the inherent grammatical structure from the sentences was removed by randomizing the word orders within the sentences \cite{xavier:2013:RT}. Such a test will also help us to have an insight on what the model is actually learning and whether the model is overfitting or not. 

\paragraph{Word2Vec-$\theta$RARes model:}In this experiment, the corpus-462 with the scrambled sentences (i.e. in the absence of any grammatical structure) was presented to Word2Vec-$\theta$RARes model. Keeping all the conditions and the reservoir parameters same as used in Experiment-2, a 10-fold cross-validation was performed with five model instances. The cross-validation errors obtained in the Experiment-2 on the corpus-462 with inherent grammatical structure can then be compared with the cross-validation error obtained while using scrambled corpus. If the model is not overfitting and learning from the grammatical structure, then the model should not perform better on the corpus with scrambled sentences (i.e. in the absence of grammatical structure). However, in the case of overfitting the generalization effect on the corpus should not vary much both in presence and absence of grammatical structure \cite{xavier:2013:RT}. 

\paragraph{Word2Vec-$\theta$RARes model:} Like Word2Vec-$\theta$RARes model, the Word2Vec-ESN classifier was also presented with the scrambled sentences of corpus-462. Keeping all the conditions and the reservoir parameters same as used in Experiment-2, a 10-fold cross-validation was performed with five instances of the Word2Vec-ESN classifier. The Word2Vec-ESN classifier was also operated with scrambled corpus in configuration-1 and configuration-2 as described in Experiment-2.

\subsection{Experiment-4: Effect of Reservoir size} \label{exp-4}

The most important hyperparameter which affects the performance of the model is the size of the reservoir (i.e. $N_{x}$ = number of neurons in the reservoir). The addition of neurons in the reservoir is also computationally inexpensive because the read-out weights ($W^{out}$) scales linearly with the number of neurons in the reservoir \cite{esn:learn_gs}. So, in order to determine the effect of reservoir size on the performance of the Word2Vec-$\theta$RARes model and Word2Vec-ESN classifier, the simulations were performed over a range of reservoir sizes \footnote[1]{Reservoir sizes explored in Word2Vec-$\theta$RARes model: [90, 291, 493, 695, 896, 1098, 1776, 2882, 3988, 5094]} \footnote[2]{Reservoir sizes explored in Word2Vec-ESN classifier: [50, 100, 250, 400, 600, 800, 1050, 1600, 2250, 3320, 3860, 4500]} using five reservoir instances. 

\paragraph{Word2Vec-$\theta$RARes model:} With Word2Vec-$\theta$RARes model, the simulations were performed on a range of reservoir size\footnotemark[1] in both SCL and SFL modes using the same optimal reservoir parameters as used in the Experiment-2 i.e. $SR = 2.4$, $IS = 2.5$ and $LR = 0.07$ in SCL mode and $SR = 2.2$, $IS = 2.3$ and $LR = 0.13$ in SFL mode. 

\paragraph{Word2Vec-ESN classifier:} To see the effect of reservoir size on Word2Vec-ESN classifier, simulations were performed on a range of reservoir sizes \footnotemark[2], in both the configuration-1 and configuration-2 of Word2Vec-ESN classifier as described in Experiment-2. 

\subsection{Experiment-5: Scalability of the model} \label{exp-5}

In order to investigate the effect of corpus size and determine the scaling capability of the model, the extended corpus-90582 (see section \ref{corpora}) was used for this experiment. As this corpus also contains $12\%$ of ambiguous sentences which impedes the learning and generalization of the model, this experiment will also validate the model's ability to process the abmbiguous sentences. Thus, in order to study the scaling capabilty of the model with different corpus size, 6 sub-corpora were created by randomly sampling $6\%$, $12\%$, $25\%$, $50\%$, $75\%$, $100\%$ of sentences from the orginal corpus of 90582 sentences \cite{xavier:2013:RT}.

\paragraph{Word2Vec-$\theta$RARes model:} Each of the generated sub-corpora was exposed to the Word2Vec-$\theta$RARes model, and a 2-fold cross-validation was performed. The model was trained on half the sub-corpora size and tested on remaining half. The second half used for testing was then used to train the model and tested on the first half, used for training previously. This experiment was performed with the Word2Vec-$\theta$RARes model in both the learning modes i.e. SCL and SFL mode, using five reservoir instances each with 5000 neurons. All the other parameters were kept identical to the Experiment-2. 

\subsection{Experiment-6: Generalization on new corpus} \label{exp-6}

One may argue that the previously used corpora (corpus-462 and corpus-90582), which were artificially constructed using context-free grammar may add a bias to the model. Thus making it easier for the model to learn and generalize on these corpora for TRA task. To answer this question, in this experiment, the corpus-373 (see section \ref{corpora}) containing the instructions collected from humans in a Human-Robot Interaction (HRI) study of language acquisition was used. 

\paragraph{Word2Vec-$\theta$RARes model:} To test the generalization of Word2Vec-$\theta$RARes model on corpus-373, the model was operated in SCL mode and tested using the LoO cross-validation method. The SCL mode and the LoO method were chosen so that results of this experiment can be compared with that of obtained using  $\theta$RARes model \cite{tra:xavier_hri}. Also, to make the results of both the model deterministically comparable, topologically modified coding was not utilized for this experiment (see section \ref{sec:w2v-esn_model}) and simulations were performed using ten reservoir instances. 

For this experiment, reservoir parameter space was not explored to identify optimal reservoir parameters, but instead, the previously optimized parameters obtained on corpus-462 with topological modified coded meaning was used i.e. $SR = 2.4$, $IS = 2.5$ and $LR = 0.07$ (see section \ref{grid_search}). Doing so will enable us to test the robustness of these model parameters on the new corpus i.e. corpus-373.

\subsection{Experiment-7: Effect of Word2Vec word dimensions} \label{exp-6}

In all the previous experiments, the distributed word vectors of 50 dimension were used. Mikolov et al. \cite{w2v:mikolov_2013_efficient, w2v:mikolov_2013_distributed} showed that the word vectors of higher dimensions (e.g. 300 in their case) perform better on word analogy task. Thus in this experiment, the effect distributed word vector dimensions on the TRA task will be explored. This experiment was only performed on Word2Vec-$\theta$RARes language model.

\paragraph{Word2Vec-$\theta$RARes model:} Recall that in section \ref{get_word_embeddings}, six Word2Vec models were trained to generate the word vectors of 20, 30, 50, 100, 200, 300 dimensions respectively. To test the effect of these word vector dimensions on TRA task, each of these Word2Vec model was used in Word2Vec-$\theta$RARes language model. For this experiment, the corpus-373 without the topologically modified coded meaning was used (see fig. \ref{fig:model_variant_1}). Simulations were performed using ten reservoir instances each with 1000 neurons. The model was trained and tested using LoO cross-validation approach in both the SCL and SFL mode, and all the reservoir parameters were kept identical to Experiment-2.
