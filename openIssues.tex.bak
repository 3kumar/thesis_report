\chapter{Previous Work and Open Issues}\label{issues}

How does humans understand the meaning of sentence from its surface form? With this research question Hinaut et al in 2013 proposed a neuro-inspired model to process the sentence across time without having to know the sematics of the words. The model used reservoir computing approach namely Echo State Network and implemented on robotics architecture(hinaut 2013,2014) for the thematic role assingment task. This was the first time when echo state network was used for thematic role assingment task. The experiments done for TRA showed the results toward modelling of language acquisition in brain (Hinaut, Dominey 2013; Hinaut, Wermter 2014; Hinaut et al. 2015). The model was based on the cue competition hypothesis of Bates et al. which states that closed class words (e.g prepositions, articles, determiners, pronouns etc.), the order of words and prosody in a sentence encodes the grammatical structure. This principle was then utilized for thematic role assingment task.

\section{Grammatical Construction and Thematic Role Assingment}

To train the ESN, the grammatical construction of sentences was used. A grammatical construction is the mapping of the surface form of a sentence (word order) to the thematic roles or meaning(see Fig. 1). The sentences were preprocessed to form an abstract representation. The abstraction marks each word of a sentence into three kind of symbols namely Function Words (FW), Semantic Words (SW) and Infrequent Words (IW). Semantic words are the open class words like nouns, verbs, adjectives, adverbs etc. whereas Function words are closed class words like determiners, prepositions, articles, pronouns etc. Infrequent words are the words which occurs and used less frequently(see table \ref{tab:localist_representation}). Thus before giving a sentence as an input to ESN, all the semantic words are replaced with a unique token SW leaving functional words unchanged.

Considering each word as a discrete atomic symbol, the words are encoded using localist vector representation, where all elements except the current input are zeros (see table \ref{tab:localist_representation}). The localist representation of word is input to ESN along with the teacher output for training across time(see fig2). Transformation of a sentence to grammatical construction as an input to ESN certainly have an advantage of training on a small dataset. The reason is that a grammatical construction can represent several sentences just by replacing SW, IW token with any open class word and infrequent words respectively.

\begin{table}[hbtp]
\centering
\caption{Transformation of a sentence by replacing semantic words with 'SW' token and localist vector representation of words used as an input for a sentence.}
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
\textbf{Original words} & \textbf{Transformed words}  & \multicolumn{6}{c|}{\textbf{Vectors}} \\ \hline
please	&	IW 	   & 1   & 0  & 0  & 0  & 0  & 0  \\ \hline
put		&	SW     & 0   & 1  & 0  & 1  & 0  & 1  \\ \hline
the		&	the    & 0   & 0  & 1  & 0  & 0  & 0  \\ \hline
ball	&	SW     & 0   & 1  & 0  & 1  & 0  & 1  \\ \hline
on		&	on     & 0   & 0  & 0  & 0  & 1  & 0  \\ \hline
the		&	the    & 0   & 0  & 1  & 0  & 0  & 0  \\ \hline
box		&	SW     & 0   & 1  & 0  & 1  & 0  & 1  \\ \hline
\end{tabular}
\label{tab:localist_representation}
\end{table}

\subsection{Limitation of using grammatical construction}

Transforming a sentence into grammatical construction and using localist representation of each word in a sentence as an input to an ESN does not allow the network to leverage the information retrieved from semantically related words. The limitations for such an input representation mainly occurs with the ambigous sentences. A sentence is said to be ambigous if it has the same grammatical construction but different coded meaning and actual meaning. The limitation can be described below in the two ambigous examples.

\paragraph{Example 1: Ambiguous Sentences}

\begin{enumerate}[noitemsep]
\item take (SW-1) the blue (SW-2) box (SW-3) \label{eg-1:sent-1}
\item take (SW-1) the left (SW-2) box (SW-3) \label{eg-1:sent-2}
\item throw(SW-1) the green(SW-2) box(SW-3)  \label{eg-1:sent-3} 
\end{enumerate}
Consider the below three sentences having the same grammatical construction i.e. SW the SW the SW SW.

\paragraph{Training with Sentence \ref{eg-1:sent-1}:} In the Sentence \ref{eg-1:sent-1} there are three semantic words namely SW-1: take, SW-2: blue, SW-3: box. During the training the sentence is input from left to right word by word at each time step. A teacher output (possible role or label) for each semantic word is also provided as described below where, A: Action, O: Object, C: Color, I: Indicator. During the training ESN learns that second semantic word represents a color.

\begin{table}[hbtp]
\centering
\begin{minipage}{1.5in}
\begin{tabular}{|c|c|c|c|}
\hline
\multicolumn{4}{|c|}{\textbf{SW-1}}   \\ \hline
\textit{\textbf{A}}    & O & C & I 	\\ \hline
\textit{\textbf{take}} &   &   &  		\\  \hline
\end{tabular}
\end{minipage}
\begin{minipage}{1.5in}
\begin{tabular}{|c|c|c|c|}
\hline
\multicolumn{4}{|c|}{\textbf{SW-2}}                    \\ \hline
A	& O & \textit{\textbf{C}}    & I  \\ \hline
	&   & \textit{\textbf{blue}} & 	\\ \hline
\end{tabular}
\end{minipage}
\begin{minipage}{1.5in}
\begin{tabular}{|c|c|c|c|}
\hline
\multicolumn{4}{|c|}{\textbf{SW-3}}                                  \\ \hline
A	& \textit{\textbf{O}}   & C	& I \\ \hline
 	& \textit{\textbf{box}} &  	&   \\ \hline
\end{tabular}
\end{minipage}
\end{table}

\paragraph{Training with Sentence \ref{eg-1:sent-2}:} In the Sentence \ref{eg-1:sent-2} we have three semantic words; SW-1: take, SW-2: left, SW-3: ball. Both the sentences (\ref{eg-1:sent-1} and \ref{eg-1:sent-2}) differs only in the second semantic word. Now suppose we train this sentence with a teacher output as follows where symbols A, O, C and I have the same meaning as described above.

\begin{table}[hbtp]
\centering
\begin{minipage}{1.5in}
\begin{tabular}{|c|c|c|c|}
\hline
\multicolumn{4}{|c|}{\textbf{SW-1}}   \\ \hline
\textit{\textbf{A}}    & O & C & I \\ \hline
\textit{\textbf{take}} &   &   &  \\  \hline
\end{tabular}
\end{minipage}
\begin{minipage}{1.5in}
\begin{tabular}{|c|c|c|c|}
\hline
\multicolumn{4}{|c|}{\textbf{SW-2}}                    \\ \hline
A       & O & C    & \textit{\textbf{I}} \\ \hline
 &   &  & \textit{\textbf{left}} \\ \hline
\end{tabular}
\end{minipage}
\begin{minipage}{1.5in}
\begin{tabular}{|c|c|c|c|}
\hline
\multicolumn{4}{|c|}{\textbf{SW-3}}                                  \\ \hline
A                  & \textit{\textbf{O}}   & C                  & I \\ \hline
 & \textit{\textbf{box}} & \textit{\textbf{}} &   \\ \hline
\end{tabular}
\end{minipage}
\end{table}

As both the sentences used for training have the same grammatical constructions, the ESN learning from Sentence \ref{eg-1:sent-1} is disrupted after training with Sentence \ref{eg-1:sent-2}.This is because the second semantic word in Sentence \ref{eg-1:sent-2} is trained to be an \textit{Indicator} whereas it was trained for \textit{Color} in the Sentence \ref{eg-1:sent-1}. Such kind of ambiguous examples having the same surface form but different coded meaning (roles assignment) and actual meaning, drops the learning ability of the network. We believe that this problem is mainly because each input words are treated as a discrete atomic symbol, which does not carry any semantic information about the input words.

\paragraph{Testing with Sentence \ref{eg-1:sent-3}:} Now, when we use the Sentence \ref{eg-1:sent-3} for testing, the network suggests two probabilities activations for the second semantic (SW-2) word \textit{green} i.e. being an \textit{Indicator} and a \textit{color} as shown below. It becomes difficult for the network to resolve this ambiguity and assign the appropriate role. Although the word \textit{green} in the test Sentence \ref{eg-1:sent-3} and the word \textit{blue} in the training sentence \ref{eg-1:sent-1} are semantically related i.e. both are colors. The network was not able to utilize this information as words were input to the in discrete form for training. Thus by using localist vector representation we are depriving the ESN from the semantic information carried out by a words in a language.

\begin{table}[hbtp]
\centering
\begin{minipage}{1.5in}
\begin{tabular}{|c|c|c|c|}
\hline
\multicolumn{4}{|c|}{\textbf{SW-2}}  \\ \hline
A	& O & \textit{\textbf{C(0.5)}}   & \textit{\textbf{I(0.5)}} \\ \hline
	&   & \textit{\textbf{green}} 	  & \textit{\textbf{green}} \\ \hline
\end{tabular}
\end{minipage}
\end{table}

\noindent Similary the following three sentences also have the same grammatical construction i.e. The SW is SW to SW.

\begin{enumerate}[noitemsep,label=(\roman*)]
\item The chicken(SW-1) is cooked(SW-2) to eat(SW-3) \label{eg-1:sent-i}
\item The ball(SW-1) is given(SW-2) to John(SW-3) \label{eg-1:sent-ii}
\item The book(SW-1) is taken(SW-2) to John(SW-3) \label{eg-1:sent-iii}
\end{enumerate}

Clearly we can see that the third semantic word is a \textit{predicate} in Sentence \ref{eg-1:sent-i} and \textit{noun} in Sentence \ref{eg-1:sent-ii}. Thus if network is trained on sentences \ref{eg-1:sent-i} and \ref{eg-1:sent-ii} and tested on sentence \ref{eg-1:sent-iii} the network is not able to resolve the ambiguity for the SW-3, which possible leads to wrong labelling of the word.

\paragraph{Example 2: Ambiguous and Polysemous Sentences}

\begin{enumerate}[noitemsep]
\item John (SW-1) books (SW-2) the ticket (SW-3) to London (SW-4) \label{eg-2:sent-1}
\item John (SW-1) read (SW-2) the books (SW-3) to learn (SW-4) \label{eg-2:sent-2}
\end{enumerate}

Both the above sentences share the same grammatical construction i.e. SW SW the SW to SW.

\paragraph{Training ESN on sentence \ref{eg-2:sent-1}} Training the ESN on the first sentence, the fourth semantic word \textit{London} is trained to be as a \textit{location}.

\paragraph{Testing ESN on sentence \ref{eg-2:sent-2}} Testing the ESN with the second sentence, the fourth semantic word \textit{learn} will be assigned the role of a location (as network was trained for this only) whereas it is actually a \textit{predicate}.
The reason for such problem is that each semantic word is considered as an unique token without taking into consideration semantics of an individual word. Hence the network could not exploit the semantic information of the words during the role assignment.

\paragraph{Issue with Polysemous words:} In the first sentence, the second semantic word \textit{books} is the predicate and describe the action of making reservation, whereas the same word (books) in the second sentence is an \textit{object}, and represents a book which we read. Although both words are same but they represent different meanings depending on the context. These kind of words with different meaning are also known as polysemous words. Semantic ambiguities because of polysemous words is hard to resolve with localist vector representation where each word is treated as a unique identifier. To resolve such kind of semantic ambiguities, distributed embedding of words can be useful as they are learned using the the contextual information of words.

\subsection{Research Hypothesis}

Use of grammatical construction as an input to the ESN have produced the promising results for the thematic role assignment task. Grammatical constructions were input to the system using localist vector representation but the ability of training the ESN with the distributed word embedding was left unexplored. Thus we hypothesize that using a distributed word embedding (e.g. Word2Vec word embeddings) could possibly resolve the problems described in the above mentioned two examples by taking into consideration the fact that they capture and encode semantic and syntactic information of words (Mikolov et al. 2013b). Also by using distributed embedding, performance of the network for thematic role assignment can be enhanced by avoiding disruption caused by unrelated semantic words for the sentences with the same grammatical constructions. Another advantage of distributed word embedding observed over localist vector representation is that the multidimensional distributed vector representation of semantically related words remains close in neighborhood within words embedding space (see Fig. 3). This allows network to learn this dynamics from the input distributed representation of the words when trained on sentences. This semantic grouping of words is not possible in localist vector encoding as words are represented as discrete atomic symbols here.

Consider using distributed embedding of words for both the sentences in Example 2 as an input to ESN. The distributed embedding of the word \textit{London} (sentence 1, Example 2) encodes that it represents a location along with several other semantic information. Similarly, the distributed embedding of word “learn” (sentence 2, Example 2) also encodes that it is a predicate along with other semantic information. When these distributed embedding is used as an input to train the network, the internal state of the network is not disrupted by the sentences having the same surface form, but different semantic words. This is because, the semantic information about the word is exposed to network and learned by it. Hence, the proper roles are assigned to the words although the sentence have the same surface form.
