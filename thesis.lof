\select@language {english}
\select@language {english}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces \textbf {Word2vec Semantic regularities:} Description goes here.\relax }}{8}{figure.caption.7}
\contentsline {figure}{\numberline {2.2}{\ignorespaces \textbf {Word2Vec Translation Property:} Description goes here [ESN9]\relax }}{8}{figure.caption.8}
\contentsline {figure}{\numberline {2.3}{\ignorespaces \textbf {Architecture of classical ESN:} The reservoir is the recurrent neural network with N units and initialized with random connection. The reservoir is provided input from the input u(n) and y(n) from the input and output neurons respectively. The input weights Win from input and Wback from output neurons are also randomly initialized and stays static during learning. The output weight from reservoir to output unit is learned by the network.[ESN9]\relax }}{10}{figure.caption.9}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces \textbf {Architecture of classical ESN:} The reservoir is the recurrent neural network with N units and initialized with random connection. The reservoir is provided input from the input u(n) and y(n) from the input and output neurons respectively. The input weights Win from input and Wback from output neurons are also randomly initialized and stays static during learning. The output weight from reservoir to output unit is learned by the network.[ESN9]\relax }}{20}{figure.caption.15}
\contentsline {figure}{\numberline {4.2}{\ignorespaces \textbf {Word2Vec-ESN Model Variant-1:} The figure shows the processing of a sentence by the model variant at time step 1. Nouns and verbs (specified in orange and green respectively) are stored in a memory stack for interpretating coded meaning. The word 'John' is input to word2vec model which generate a word vector of $E_{v}$ dimensions. The output vector is then input to ESN for further processing. During training the readout units are presented with the coded meaning of the input sentence(i.e. N1-A1: Noun-1 is Agent of verb 1, N2-O1: Noun-2 is object of verb 2). In testing, the readout units codes the predicted meaning of input sentence. The meaning: hit(John,ball,--) is decoded by mapping the thematic roles predicted by readout neurons with nouns and verbs from memory stack. Adapted from \cite {xavier:2013:RT}\relax }}{21}{figure.caption.16}
\contentsline {figure}{\numberline {4.3}{\ignorespaces \textbf {Word2Vec-ESN Model Variant-2:} The figure shows the process of a sentence in model variant-2 at time step 1. At any instant of time an argument (current word, marked in orange) and predicate (verb,marked in green) is input to the model. Word2Vec model generates the word vectors of $E_{v}$ dimensions which are then cocatenated to form a $2 \times E_{v}$ dimensions(shown in orange and green color). ESN takes the resultant vector for further processing. During learning, the readout neurons are presented with the role of input word (i.e. A (Agent)). The read-out weights (shown in dashed line) are learned during training. During testing the readout unit codes the role of input words, which are then accumulated and decoded to meaning \textit {hit(John,ball,--)} at the end of sentence. Inspired from \cite {xavier:2013:RT} \relax }}{24}{figure.caption.20}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces \textbf {Normalized Confusion matrix:} Description goes here.\relax }}{32}{figure.caption.31}
\contentsline {figure}{\numberline {5.2}{\ignorespaces \textbf {Effect of reservoir size on cross validation errors on Model Varinat-1:} Description goes here.\relax }}{34}{figure.caption.34}
\contentsline {figure}{\numberline {5.3}{\ignorespaces \textbf {Effect of reservoir size on cross validation errors on Model Variant-2:} Description goes here.\relax }}{35}{figure.caption.35}
\contentsline {figure}{\numberline {5.4}{\ignorespaces \textbf {Effect of corpus size on cross validation errors:} Description goes here.\relax }}{35}{figure.caption.36}
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
