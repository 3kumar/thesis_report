\select@language {english}
\select@language {english}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces The CBOW model}}{6}{figure.caption.6}
\contentsline {figure}{\numberline {2.2}{\ignorespaces The Skip-gram model}}{7}{figure.caption.7}
\contentsline {figure}{\numberline {2.3}{\ignorespaces Word2vec semantic regularities}}{9}{figure.caption.9}
\contentsline {figure}{\numberline {2.4}{\ignorespaces Word2Vec language translation property}}{9}{figure.caption.10}
\contentsline {figure}{\numberline {2.5}{\ignorespaces \textbf {Architecture of classical ESN:} The reservoir is the recurrent neural network with N units and initialized with random connection. The reservoir is provided input $u(n)$ to the input layer and teacher layer $y(n)$ are pushed output neurons respectively during training. The input to reservoir weights ($W^{in}$), output neurons to reservoir ($W^{back}$, optional and depends on task) and reservoir to reservoir weight ($W^{res}$) from are also randomly initialized and stays static during learning. The output weight from reservoir to output unit are the only weights learned by the network during trainging.[ESN9]\relax }}{11}{figure.caption.11}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces \textbf {Architecture of Word2Vec-ESN model:} The model takes the words of a sentence as an input across time. Word2Vec model generates the distributed vector representation of the input word. The generated word vector is then used by ESN for further processing and learns to predict the thematic roles of the input sentence.\relax }}{21}{figure.caption.24}
\contentsline {figure}{\numberline {4.2}{\ignorespaces \textbf {Word2Vec-ESN Model Variant-1:} The figure shows the processing of a sentence by the model variant at time step 1. Nouns and verbs (specified in orange and green respectively) are stored in a memory stack for interpretating coded meaning. The word 'John' is input to word2vec model which generate a word vector of $E_{v}$ dimensions. The output vector is then input to ESN for further processing. During training the readout units are presented with the coded meaning of the input sentence(i.e. N1-A1: Noun-1 is Agent of verb 1, N2-O1: Noun-2 is object of verb 2). In testing, the readout units codes the predicted meaning of input sentence. The meaning: hit(John,ball,--) is decoded by mapping the thematic roles predicted by readout neurons with nouns and verbs from memory stack. Adapted from \cite {xavier:2013:RT}\relax }}{23}{figure.caption.25}
\contentsline {figure}{\numberline {4.3}{\ignorespaces \textbf {Word2Vec-ESN Model Variant-2:} The figure shows the process of a sentence in model variant-2 at time step 1. At any instant of time an argument (current word, marked in orange) and predicate (verb,marked in green) is input to the model. Word2Vec model generates the word vectors of $E_{v}$ dimensions which are then cocatenated to form a $2 \times E_{v}$ dimensions(shown in orange and green color). ESN takes the resultant vector for further processing. During learning, the readout neurons are presented with the role of input word (i.e. A (Agent)). The read-out weights (shown in dashed line) are learned during training. During testing the readout unit codes the role of input words, which are then accumulated and decoded to meaning \textit {hit(John,ball,--)} at the end of sentence. Inspired from \cite {xavier:2013:RT} \relax }}{26}{figure.caption.29}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces \textbf {Normalized Confusion matrix when using word2vec word embeddings:} The confusion matrix with true roles (in rows) and predicted roles (in columns). The top-left to bottom-right diagonal shows the percentage of values predicted correctly.Everything other than this diagonal represents the incorrect prediction of roles. Model identified almost all words labelled as Recipient , Verb and No Role and made some error in predicting Agent and Object. The results were obtained with reservoir of 1000 neurons and 10 fold-cross validation.\relax }}{36}{figure.caption.42}
\contentsline {figure}{\numberline {5.2}{\ignorespaces \textbf {Effect of reservoir size on cross validation errors on Model Variant-1:} Description goes here.\relax }}{37}{figure.caption.45}
\contentsline {figure}{\numberline {5.3}{\ignorespaces \textbf {Effect of reservoir size on classification scores of Model Varinat-2:} Description goes here.\relax }}{38}{figure.caption.46}
\contentsline {figure}{\numberline {5.4}{\ignorespaces \textbf {Effect of corpus size on cross validation errors:} Description goes here.\relax }}{39}{figure.caption.47}
\contentsline {figure}{\numberline {5.5}{\ignorespaces \textbf {Effect of corpus size on cross validation errors using localist word vector as reported in [ref?]:} Description goes here.\relax }}{39}{figure.caption.48}
\contentsline {figure}{\numberline {5.6}{\ignorespaces \textbf {Effect of corpus size on cross validation errors using localist word vector as reported in [ref?]:} Description goes here.\relax }}{41}{figure.caption.49}
\contentsline {figure}{\numberline {5.7}{\ignorespaces \textbf {Effect of corpus size on cross validation errors using localist word vector as reported in [ref?]:} Description goes here.\relax }}{41}{figure.caption.50}
\contentsline {figure}{\numberline {5.8}{\ignorespaces \textbf {Effect of corpus size on cross validation errors using localist word vector as reported in [ref?]:} Description goes here.\relax }}{42}{figure.caption.51}
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
