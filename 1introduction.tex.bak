\chapter{Introduction}\label{introduction}

Thematic Role Assignment (TRA) is a supervised learning problem which aims to identify events and its participants from a sentence and determine \textit{"Who did what to whom"}. In other words assigning roles to words (arguments) in a sentence with respect to a verb (predicate). The role typically includes agent, object, recipient etc. For example, in the sentence \textit{"the dog that gave the rat to the cat was hit by the man"}, the first noun \textit{'dog'} is the agent of verb \textit{'gave'} and object of verb \textit{'hit'}. In Natural Language Processing terminology (NLP) the problem is studied under the name of Semantic Role Labelling (SRL). Hence, TRA or SRL is a form of simplistic semantic parsing which aims to determine the predicate-argument structure for a verb in the given sentence \cite{end-to-end}. Understanding the semantics of the text plays an important intermediate step in a wide range of real-world applications such as machine translation \cite{srl:machine_translation}, information extraction \cite{srl:info_extraction:hri}, sentiment analysis \cite{srl:sentiment:wang}, document categorization \cite{srl:text_categorization:persson}, human-robot interaction \cite{tra:xavier_hri,srl:info_extraction:hri} etc.

\section{Previous Work}

Many successful traditional systems consider SRL as a multiclass classification problem and uses linear classifier such as Support Vector Machines (SVM) to tackle the problem \cite{Koomen:2005,srl:pradhan:2004,pradhan:2005}. These systems were based on pre-defined feature templates derived from syntactic information obtained by parsing and producing parse trees of the sentences in the training corpus. However, in an analysis, it was found that the use of syntactic parser certainly leads to degradation of thematic roles predictions \cite{pradhan:2005}. Also, designing of feature templates needs a lot of heuristics and is a time consuming process. The pre-defined features are often required to be iteratively modified depending on how the system performs with the selcted features. The feature templates are often required to be re-designed when the task is to be performed in different languages, corpus or when the data distribution is changed \cite{end-to-end}.

In order to avoid manual feature engineering, SRL task was also attempted with neural network models. Collobert et al. \cite{srl:collobert:2011} first attempted to build an end-to-end system without using parsing and using a neural model with word embeddings layer, Convolutional Neural Network (CNN) and Conditional Random Field (CRF). The model was less successful as CNN cannot employ long-term dependencies within a sentence since it can only take into account the words in limited context \cite{end-to-end}. However, to increase the model performance they also resorted to using syntactic features by using parse trees of charnink parser \cite{charniak_parser:2000}.

Recurrent Neural Networks (RNN) has also been used for wide range of NLP task and also recently with Echo State Network (ESN), a variant of RNN. The tasks used were diverse from predicting next word given the previous words to learning grammatical structures \cite{esn:learn_gs}. RNN makes use of sequential information and acts as a memory unit and captures the information processed in the past \cite{rnn:elman:1990}. The ESN have several advantages over simple RNN. First, ESNs are capable of modeling long-term dependencies in the sentence. Second, while processing long sequence the gradient parameter vanishes or explodes in simple RNNs \cite{rnn:gradiant_problem:bengio}. Third, unlike simple RNN, ESNs are computationally cheap as the recurrent layer (reservoir) in ESN is randomly initialized and only the connections from recurrent layer to read-out layers are learned \cite{esn:NIPS:2003, esn:practical_guide}. These advantages of ESN over RNN makes it a good choice to be used for TRA task.

Hinaut et al. \cite{xavier:2013:RT} proposed a generic neural network architecture, $\theta RARes$ model, using Recurrent Neural Network based on reservoir computing approaches, namely Echo State Network to solve TRA task. The proposed architecture models the language acquisition in the brain and provided a robust and scalable implementation on robotics architecture \cite{xavier:2013:RT,tra:xavier_hri}.  The model is based on the notion of grammatical construction: mapping of word order (surface form) to its meaning. They first transformed the raw sentences by replacing the semantic words (nouns, verbs etc.) with a unique token `SW' then the sentences are input to model sequentially, word by word across time along with the coded meaning (i.e. thematic roles of semantic words) of the input sentence for training. The model learns the thematic roles of all the semantic words in the input sentence during training. During testing, the model predicts the coded meaning of the previously unseen sentences. See chapter \ref{issues} for more details about $\theta RARes$ model.

\section{Motivation and Hypothesis}

Like many other traditional NLP systems, Hinaut et al. \cite{xavier:2013:RT} also treated words as discrete atomic symbols and used localist vector representation of words as an input. Treating each word as a discrete symbol does not provide any relational information to the model which may exist between two words. For example, if words \textit{`pink'} and \textit{`red'} are represented using localist representation with vectors [1,0] and [0,1] respectively, the semantic relationship (i.e. both are colors) between these two words is lost \cite{w2v:tensor_flow}. Although, replacing the semantic words with `SW' token makes it possible to train the model on a small corpus as the `SW' token can be replaced with different semantic words (nouns, verbs etc.) to form a sentence. Whereas, the `SW' token in itself does not carry any semantics and thus does not allow the model to take into account the semantics of the words. This makes it difficult for the model to learn thematic roles for sentences. We discuss in more detail about the possible limitation of this model later in Chapter \ref{issues}.

Motivated by the limitations of localist representation of words and transformation of raw sentences into its abstract form by replacing semantic words with `SW' token described above, this work hypothesize that the use of distributed word representations, which can capture the syntactic and semantic relationship of words, could possibly improve the performance of the model on TRA task. One such model for learning distributed word embeddings was proposed by Mikolov et al. \cite{w2v:mikolov_2013_distributed} widely known as the Word2Vec model. Word2Vec model learns high quality, low-dimensional vector representation of words from a large corpus in an unsupervised way. The resulting word vector of this model encodes the semantics of the words. The model learns the word embeddings by taking into account the context (neighbouring) words. The obtained word embeddings captures several language regularities and patterns \cite{w2v:language_similarities, w2v:mikolov_2013_distributed} and can be observed by performing the linear operations on the word vectors. For example, \textit{vector(`king') - vector(`man') + vector(`woman') $\approx$ vector(`queen')}. Unlike other neural network models \cite{w2v:glove, srl:collobert:2008, word_vec:turian:2010, word_vec:hinton:2009} for obtaining word embeddings, training Word2Vec is computationally cheap and efficient \cite{w2v:mikolov_2013_distributed}. Training of word2vec model and properties of resulting distributed word embeddings will be discussed later in more detail in Chapter \ref{basics}.  

\section{Proposed Models}

This work, proposes an end-to-end neural language model for thematic role assignment. The model is named as \textit{Word2Vec-ESN} language model. The Word2Vec-ESN model is a combination of word2vec model and ESN. The word2vec model is trained on a general purpose unlabeled dataset (e.g. Wikipedia) prior to use of the model for TRA task. The model receives the raw sentences sequentially and process a sentence word by word generating. The word2vec unit being the first unit in the model, takes the input word to generate the distributed word embeddings. The generated word vector by word2vec model can then be used by ESN for learning thematic roles of the input sentences. Note that the proposed model is an extension of $\theta RARes$ model \cite{xavier:2013:RT}, where unlike the latter the raw sentences are not transformed to grammatical forms and word2vec word vectors are used over localist word representation as an input to ESN. 

Apart from Word2Vec-ESN model, this thesis also propose a variant of Word2Vec-ESN language model which only differs from the original in the way, the sentences are processed and results are evaluated. Thus in this model variant, the inputs and outputs of the model are changed. The input feature to this model variant is the current word and the predicate, with respect to which it is processed. The output units encode the possible role (e.g. Predicate, Agent, Object, Recipient and No Role) of the input argument-predicate pairs, unlike the original model where output units encode the possible thematic roles of all semantic words in the input sentences. For the evaluation of this model variant, the metrics proposed for Conll-2004 \cite{,conll:2004} and CoNLL-2005 \cite{conll:2005} SRL task is used. Both Word2Vec-ESN language model and its variant are discussed in more detail in Chapter \ref{approach}.

There are several neural network based model to obtain the word the word embeddings \cite{w2v:glove, srl:collobert:2008, word_vec:turian:2010, word_vec:hinton:2009} but a systematic comparison of them on TRA task is beyond the scope of this work. To this date, there is no research conducted with this combination of word2vec model and ESN. This also makes this study novel.

\section{Outline}

This thesis is organized as follows. The Chapter \ref{basics}, describes the basics of word2vec model and echo state network model. The chapter also describe in detail the training of word2vec model and properties of resulting word vectors. Also, this chapter describes training and control parameters of ESN. Chapter \ref{issues} looks upon related work with primary focus on neural network $\theta RARes$ language model, its limitations, the motivation and hypothesis for the current work. Subsequently chapter \ref{approach} proposes the Word2Vec-ESN model and its variant. This chapter also describes the implementation and the metrics used to evaluate the performance of the proposed model and its variant. Chapter \ref{results} first describes the corpora used and then the experimental setup along with the experiments performed using the proposed model. We will then see the results obtained using proposed model on TRA task. In this chapter, we also compare the results of Word2Vec-ESN model with the results obtained from  $\theta RARes$ model and analyze them. Finally, in the chapter \ref{conclusion} we conclude the study and the possible future work.