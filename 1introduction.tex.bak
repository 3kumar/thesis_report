\chapter{Introduction}\label{introduction}

Thematic Role Assignment (TRA) is a supervised learning problem which aims to identify events and its participants in a sentence and determine \textit{"Who did what to whom"}. In other words assigning roles to words (arguments) in a sentence with respect to a verb (predicate). The role typically includes an agent, object, recipient, etc. For example, in the sentence \textit{"the dog that gave the rat to the cat was hit by the man"}, the first noun \textit{'dog'} is the agent of the verb \textit{`gave'} and object of the verb \textit{`hit'}. In Natural Language Processing terminology (NLP) the problem is studied under the name of Semantic Role Labelling (SRL). Hence, TRA or SRL is a form of simplistic semantic parsing which aims to determine the predicate-argument structure for a verb in the given sentence \cite{end-to-end}. Understanding the semantics of the text plays an important intermediate step in a wide range of real-world applications such as machine translation \cite{srl:machine_translation}, information extraction \cite{srl:info_extraction:hri}, sentiment analysis \cite{srl:sentiment:wang}, document categorization \cite{srl:text_categorization:persson}, human-robot interaction \cite{tra:xavier_hri,srl:info_extraction:hri} etc.

\section{Previous Work}

Many successful traditional systems consider SRL as a multiclass classification problem and use linear classifier such as Support Vector Machines (SVM) to tackle the problem \cite{Koomen:2005,srl:pradhan:2004,pradhan:2005}. These systems are based on pre-defined feature templates derived from syntactic information obtained by parsing and producing parse trees of the sentences in the training corpus. However, in a study it was found that the use of syntactic parser certainly leads to degradation of thematic roles predictions \cite{pradhan:2005}. Also, designing of feature templates needs a lot of heuristics and is a time-consuming process. The pre-defined features are often required to be iteratively modified depending on how the system performs with the selected features. The feature templates are often required to be re-designed when the task is to be carried out in different languages, corpora or when the data distribution is changed \cite{end-to-end}.

In order to evade manual feature engineering, SRL task was additionally endeavored with neural network models. Collobert et al. \cite{srl:collobert:2011} first attempted to build an end-to-end system without using parsing and using a neural model with word embeddings layer, Convolutional Neural Network (CNN) and Conditional Random Field (CRF). The model was less successful as CNN cannot employ long-term dependencies within a sentence since it can only take into account the words in limited context \cite{end-to-end}. However, to increase the model performance they also resorted to using syntactic features by using parse trees of Charniak parser \cite{charniak_parser:2000}.

Recurrent Neural Networks (RNN) has also been used for a wide range of NLP tasks and also recently with Echo State Network (ESN), a variant of RNN. The tasks performed were diverse from predicting the next word given the previous words to learning grammatical structures \cite{esn:learn_gs}. A RNN makes use of sequential information and acts as a memory unit and captures the information processed in the past \cite{rnn:elman:1990}. The ESN has several advantages over simple RNNs. First, ESNs are capable of modeling long-term dependencies in the sentence. Second, while processing long sequence, the gradient parameter vanishes or explodes in simple RNNs \cite{rnn:gradiant_problem:bengio}. Third, unlike simple RNNs, ESNs are computationally cheap as the recurrent layer (reservoir) in ESN is randomly initialized and only the connections from recurrent layer to read-out layers are learned \cite{esn:NIPS:2003, esn:practical_guide}. These advantages of ESN over RNN makes it an excellent choice to be used for the TRA task.

Hinaut et al. \cite{xavier:2013:RT} proposed a generic neural network architecture, $\theta RARes$ model, using Recurrent Neural Network based on reservoir computing approaches, namely Echo State Network to solve TRA task. The proposed architecture models the language acquisition in the brain and provides a robust and scalable implementation on robotics architecture \cite{xavier:2013:RT,tra:xavier_hri}.  The model is based on the notion of grammatical construction: mapping of word order (surface form) to its meaning. They first transformed the raw sentences by replacing the semantic words (nouns, verbs, etc.) with a unique token `SW' then the sentences are input to the model sequentially, word by word across time along with the coded meaning (i.e. thematic roles of semantic words) of the input sentence for training. The model learns the thematic roles of all the semantic words in the input sentence during training. During testing, the model predicts the coded meaning of the previously unseen sentences. See chapter \ref{issues} for more details about $\theta RARes$ model.

\section{Motivation and Hypothesis}

Like many other traditional NLP systems, Hinaut et al. \cite{xavier:2013:RT} also treated words as discrete atomic symbols and used localist vector representation of words as an input. Treating each word as a discrete symbol does not provide any relational information to the model which may exist between two words. For example, if words \textit{`pink'} and \textit{`red'} are represented using localist representation with vectors [1,0] and [0,1] respectively, the semantic relationship (i.e. both are colors) between these two words is lost \cite{w2v:tensor_flow}. Although, replacing the semantic words with a `SW' token makes it possible to train the model on a small corpus as the `SW' token can be replaced with different semantic words (nouns, verbs, etc.) to form a sentence. Whereas, the `SW' token in itself does not carry any semantics and thus does not allow the model to take into account the semantics of the words. This makes it difficult for the model to learn thematic roles for sentences. We discuss the possible limitation of this model later in more detail in Chapter \ref{issues}.

Motivated by the limitations of localist representation of words and transformation of raw sentences into its abstract form by replacing the semantic words with a `SW' token described above, this work hypothesizes that the use of distributed word representations, which can capture the syntactic and semantic relationship of words, could improve the performance of the model on TRA task. One such model for learning distributed word embeddings was proposed by Mikolov et al. \cite{w2v:mikolov_2013_distributed} widely known as the Word2Vec model. Word2Vec model learns high quality, low-dimensional vector representation of words from a large corpus in an unsupervised way. The resulting word vector of this model encodes the semantics of the words. The model learns the word embeddings by taking into account the context (neighboring) words. The obtained word embeddings capture several language regularities and patterns \cite{w2v:language_similarities, w2v:mikolov_2013_distributed} and can be observed by performing the linear operations on the word vectors. For example, \textit{vector(`king') - vector(`man') + vector(`woman') $\approx$ vector(`queen')}. Unlike other neural network models \cite{w2v:glove, srl:collobert:2008, word_vec:turian:2010, word_vec:hinton:2009} for obtaining word embeddings, training Word2Vec is computationally cheap and efficient \cite{w2v:mikolov_2013_distributed}. Training of Word2Vec model and properties of resulting distributed word embeddings will be discussed later in more detail in Chapter \ref{basics}.  

\section{Proposed Models}

This work proposes an end-to-end neural language model for thematic role assignment. The model is named  \textit{Word2Vec-$\theta$RARes} language model. The Word2Vec-$\theta$RARes model is a combination of Word2Vec model and ESN. The Word2Vec model is trained on a general purpose unlabeled dataset (e.g. Wikipedia) prior to the use of the Word2Vec-$\theta$RARes model for TRA task. The model receives the raw sentences sequentially and processes a sentence word by word over time. The Word2Vec unit being the first unit in the model receives the input word to generate the distributed word embeddings. The generated word vector by Word2Vec model can then be used by ESN for learning the thematic roles of the input sentences. Note that the proposed model is an extension of the $\theta RARes$ model \cite{xavier:2013:RT}, where, unlike the latter, the raw sentences are not transformed to grammatical forms and word2vec word vectors are used over localist word representation as an input to ESN. 

Apart from the Word2Vec-$\theta$RARes model, this thesis also proposes a Word2Vec-ESN classifier model which only differs from the Word2Vec-$\theta$RARes model in the way the sentences are processed, and the results are evaluated. Thus in the classifier, the inputs and outputs differs from the Word2Vec-$\theta$RARes model. The input feature to this Word2Vec-ESN classifier is the current word (argument) and the predicate with respect to which it is processed. The output units encode the possible role (e.g. Predicate, Agent, Object, Recipient and No Role) of the input argument-predicate pairs, unlike the $\theta$RARes model where output units encode the possible thematic roles of all semantic words in the input sentences. For the evaluation of this Word2Vec-ESN classifier, the metrics proposed for Conll-2004 \cite{conll:2004} and CoNLL-2005 \cite{conll:2005} SRL task is used. Both Word2Vec-$\theta$RARes language model and Word2Vec-ESN classifier model are discussed in more detail in Chapter \ref{approach}.

There are several neural network based models to obtain the word the word embeddings \cite{w2v:glove, srl:collobert:2008, word_vec:turian:2010, word_vec:hinton:2009} but a systematic comparison of them on TRA task is beyond the scope of this work. To this date, there is no research conducted with this combination of word2vec model and ESN. This also makes this study novel.

\section{Outline}

This thesis is organized as follows. Chapter \ref{basics}, describes the basics of Word2Vec model and the Echo State Network model. The chapter also describes in detail the training of the Word2Vec model and properties of resulting word vectors. Also, this chapter describes training and control parameters of ESN. Chapter \ref{issues} looks upon related work with the primary focus on neural network $\theta RARes$ language model, its limitations, the motivation and hypothesis for the current work. Subsequently, chapter \ref{approach} proposes the Word2Vec-$\theta$RARes model and the Word2Vec-ESN classifier. This chapter also describes the implementation and the metrics used to evaluate the performance of the proposed models. Chapter \ref{results} first describes the corpora used and then the experimental setup along with the experiments performed using the proposed models. We will then see the results obtained using the proposed model on the TRA task. In this chapter, we also see the comparison of the results of obtained using the proposed models and the $\theta RARes$ model and analyze them. Finally, in chapter \ref{conclusion} we conclude this study and make the suggestions for  possible future work.