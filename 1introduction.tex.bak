\chapter{Introduction}\label{introduction}

Thematic Role Assignment (TRA) is a supervised learning problem which aims to identify events and its participants from a sentence and determine \textit{"Who did what to whom"}. In other words assigning roles to words (arguments) in a sentence with respect to a verb (predicate). The role typically includes agent, object, recipient etc.. For example in the sentence \textit{"the dog that gave the rat to the cat was hit by the man"}, the first noun \textit{'dog'} is the agent of verb \textit{'gave'} and object of verb \textit{'hit'}. In Natural Language Processing terminology (NLP) the problem is studied under the name of Semantic Role Labelling (SRL). Hence, TRA or SRL is a form of simplistic semantic parsing which aims to determine the predicate-argument structure for a verb in the given sentence \cite{end-to-end}. Understanding the semantics of the text plays an important intermediate step in a wide range of real-world applications such as machine translation \cite{srl:machine_translation}, information extraction \cite{srl:info_extraction:hri}, sentiment analysis \cite{srl:sentiment:wang}, document categorization \cite{srl:text_categorization:persson}, human robot interaction \cite{tra:xavier_hri,srl:info_extraction:hri} etc.

\section{Previous Work}

Many successful traditional system consider SRL as a multiclass classification problem use linear classifier such as Support Vector Machines (SVM) to tackle the problem \cite{Koomen:2005,srl:pradhan:2004,pradhan:2005}. These system were based on pre-defined feature templates derived from syntactic information obtained by parsing and producing parse trees of the sentences in the training corpus. However in an analysis it was found that the use of syntatic parser certainly leads to degradation of predictions \cite{pradhan:2005}. Also  designing of feature templates need a lot of heuristics and time. The pre-defined features are often required to be iteratively modified depending on how the system perfoms. The feature templates are often required to be re-desinged when the task is to be performed on different languages, corpus or when the data distribution is changed \cite{end-to-end}.

In order to avoid engineering manual feauture templates, SRL task was also attempted with neural network models. Collobert et al. \cite{srl:collobert:2011} first attempted to build an end-to-end system without parsing by using word embeddings and Convolutional Neural Network (CNN). The model was less successful as CNN cannot employs long term dependencies within a sentence since it can only take into account the words in limitied context \cite{end-to-end}. However to increase the model perfomance they also resorted to use syntatic features by using parse trees of charnink parser \cite{charniak_parser:2000}.

% TODO: Improve it
Recurrent Neural Networks (RNN) has been also been used for wide range NLP task and also recently with Echo State Network (ESN): a varinat of RNN. The tasks used were diverse from perdicting next word given the previous words to learning grammatical structures \cite{esn:learn_gs}.  RNN makes use of sequential information and acts as a memory unit and captures the information processed in the past \cite{rnn:elman:1990}. The ESN have several advantages over simple RNN. First, ESNs are capable of modeling long term dependencies in the sentence. Second, while processing long sequence the gradient parameter vanishes or explodes in simple RNNs \cite{rnn:gradiant_problem:bengio}. Third, unlike simple RNN ESNs are computationally cheap as in ESN the recurrent layer (reservoir) is randomly initialized and only connections from recurrent layer to read-out layers are learned \cite{esn:NIPS:2003, esn:practical_guide}. These advantages of ESN over RNN makes it good choice to be used for TRA task.

Xavier et al. \cite{xavier:2013:RT} proposed a generic neural network architecture using Recurrent Neural Network based on reservoir computing approaches, namely Echo State Network to solve TRA task. The proposed architecture models the language acquisition in brain and provided a robust and scalable implementation on robotics architecture \cite{xavier:2013:RT,tra:xavier_hri}. They called this model as $\theta RARes$. The model is based on the notion of grammatical construction: mapping of word order (surface form) to its meaning. They first transformed the raw sentences by replacing the semantic words (nouns, verbs etc.) with a unique token $'SW'$ then the sentences are input to model sequentially, word by word across time along with the coded meaning (i.e. thematic roles of semantic words) of the input sentence for training. The model learns the thematic roles of all the semantic words in the input sentence during training. During testing, the model predicts the coded meaning of the previously unseen sentences. See chapter \ref{issues} for more details about $\theta RARes$ model.

\section{Motivation and Hypothesis}

Like many other traditional NLP system they also treated words as discrete atomic symbols and used localist vector respresentaion of words as an input. Treating each word as a dicrete symbol does not provide any relational information to the model which may exist between two words. For example, if words \textit{'pink'} and \textit{'red'} are represented using localist representation with vectors [1,0] and [0,1] respectively, then the semantic relationship (i.e. both are colors) between these two words is lost \cite{w2v:tensor_flow}. Although, replacing the semantic words with $'SW'$ token makes it possible to train the model on a small corpus as the $'SW'$ token can be replaced with different semantic words (nouns, verbs etc.) to form a sentence. Whereas on the other hand the 'SW' token in iteself does not carry any semantics and thus does not allow model to take into account the semantics of the words. This makes it difficult for the model to learn thematic roles for sentences. We discuss more in detail about the limitation of this model later in Chapter \ref{issues}.

Motivated by the limitations of localist input representation of words and transformation of raw sentences into its abstract form by replacing semantic words with 'SW' token described above, we hypothesize that the use of distributed word representation which can capture the syntactic and semantic relationship of words could possibly improve the perfomance of the model on TRA task. One such model for learning distributed word vectors was proposed by Mikolov et al. \cite{w2v:mikolov_2013_distributed} widely known as Word2Vec model. Word2Vec model learn high quality, low-dimensional vector represenation of words from a large corpus in an unsupervised way[ref-two]. The resulting word vector of this model encodes semantics of words. As the model learns the embeddings by taking into account the context words, the obtained vector embeddings also encodes several language regularities and patterns[ref] and can be observed by performing linear operation on the word vector. For example, \textit{vector('king') - vector('man') + vector('woman') $\approx$ vector('queen')}. Unlike other neural network models for obtaining word embeddings, training Word2Vec is computationally cheap and efficient[ref?]. Training of word2vec model and properties of resulting distributed word embedding will be discussed in detail in Chapter \ref{basics}. 

\section{Proposed Models}

In this work, we thus propose a end-to-end system called \textit{Word2Vec-ESN} model, for TRA task. The Word2Vec-ESN model is a combination of word2vec model and ESN. The word2vec model is trained on a general purpose unlabelled dataset (e.g. wikipedia) prior to use of model for TRA task. The word2vec unit being the first unit, receives the raw sentences and generates the distributed word embeddings of the constituent words. The generated word vector by word2vec model can then be used by ESN for learning thematic roles of the input sentences. Note that the proposed model is basically a modified verision of $\theta RARes$ model \cite{xavier:2013:RT}, where unlike the latter raw sentences are not transformed to grammatical construcion and word2vec word vectors are used over localist word representation as an input to ESN. 

Apart from Word2Vec-ESN model, we also propose a variant of this model which only differs from the original in the way the sentences are processed and results are evaluated. Thus in this model variant the inputs and outputs of the model are changed. The input feautre to this model variant is the current word and the verb with respect to which it is processed. The output units encodes the possible role (e.g. predicate, agent, object, recipient and No Role) of the input words unlike the original model where output units encodes the thematic roles of all sematic words in the input sentence. For the evaluation of this model variant we used metrics (classification scores) proposed for CoNLL-2005 SRL task[ref]. Both Word2Vec-ESN model and its variant is discussed in mored detail in chapter \ref{approach}.

[TODO:Describe the overview of results here.]

\section{Scope of work}

There are several other ways of obtaining word embeddings [glove and other] but a systematic comparision of them on TRA task is beyond the scope of this work. Using word2vec model, distributed word vectors of different dimesions can be obtained and used for TRA task. Evaluating and comparing the effect of dimesnsion of these word embedding is also not the focus of this study. To this date, there is no research conducted with this combination of word2vec model and ESN. This also makes this study novel.

\section{Outline}

In the next chapter we give a description of  word2vec model and echo state network model. We also descibe in detail the training of word2vec model and properties of word2vec word vectors. Alse this chapter describes training and control parameters of ESN. The chapter \ref{issues} describes the  $\theta RARes$ model, its limitations and the motivation and hypothesis for the current work. Subsequently in chapter \ref{approach} we propose the Word2Vec-ESN model and its variant. This chapter also describes the data, implementation and evaluation metrics used in our experiments. Chapter \ref{results} contains the experiments and results performed for TRA task with proposed model along with the results. This chapter we also compares the results of Word2Vec-ESN model with the results obtained from  $\theta RARes$ model and analysis them. Finally, in the chapter \ref{conclusion} we describe the conclusion of this study and the possible future work.







