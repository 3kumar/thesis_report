\chapter{Introduction}\label{introduction}

Thematic Role Assignment (TRA) is a supervised learning problem which aims to identify events and its participants from a sentence and determine \textit{"Who did what to whom"}. In other words assigning roles to words (arguments) in a sentence with respect to a verb (predicate). The role typically includes agent, object, recipient etc.. For example in the sentence \textit{"the dog that gave the rat to the cat was hit by the man"}, the first noun \textit{'dog'} is the agent of verb \textit{'gave'} and object of verb \textit{'hit'}. In Natural Language Processing terminology (NLP) the problem is studied under the name of Semantic Role Labelling (SRL). Hence, TRA or SRL is a form of simplistic semantic parsing which aims to determine the predicate-argument structure for a verb in the given sentence \cite{end-to-end}. Understanding the semantics of the text plays an important intermediate step in a wide range of real-world applications such as machine translation \cite{srl:machine_translation}, information extraction \cite{srl:info_extraction:hri}, sentiment analysis \cite{srl:sentiment:wang}, document categorization \cite{srl:text_categorization:persson}, human-robot interaction \cite{tra:xavier_hri,srl:info_extraction:hri} etc.

\section{Previous Work}

Many successful traditional systems consider SRL as a multiclass classification problem use linear classifier such as Support Vector Machines (SVM) to tackle the problem \cite{Koomen:2005,srl:pradhan:2004,pradhan:2005}. These systems were based on pre-defined feature templates derived from syntactic information obtained by parsing and producing parse trees of the sentences in the training corpus. However, in an analysis, it was found that the use of syntactic parser certainly leads to degradation of predictions \cite{pradhan:2005}. Also,  designing of feature templates needs a lot of heuristics and time. The pre-defined features are often required to be iteratively modified depending on how the system performs. The feature templates are often required to be re-designed when the task is to be performed in different languages, corpus or when the data distribution is changed \cite{end-to-end}.

In order to avoid engineering manual feature templates, SRL task was also attempted with neural network models. Collobert et al. \cite{srl:collobert:2011} first attempted to build an end-to-end system without parsing by using word embeddings and Convolutional Neural Network (CNN). The model was less successful as CNN cannot employ long-term dependencies within a sentence since it can only take into account the words in limited context \cite{end-to-end}. However, to increase the model performance they also resorted to using syntactic features by using parse trees of charnink parser \cite{charniak_parser:2000}.

% TODO: Improve it
Recurrent Neural Networks (RNN) has been also been used for wide range NLP task and also recently with Echo State Network (ESN): a variant of RNN. The tasks used were diverse from predicting next word given the previous words to learning grammatical structures \cite{esn:learn_gs}.  RNN makes use of sequential information and acts as a memory unit and captures the information processed in the past \cite{rnn:elman:1990}. The ESN have several advantages over simple RNN. First, ESNs are capable of modeling long-term dependencies in the sentence. Second, while processing long sequence the gradient parameter vanishes or explodes in simple RNNs \cite{rnn:gradiant_problem:bengio}. Third, unlike simple RNN ESNs are computationally cheap as in ESN the recurrent layer (reservoir) is randomly initialized and only connections from recurrent layer to read-out layers are learned \cite{esn:NIPS:2003, esn:practical_guide}. These advantages of ESN over RNN makes it a good choice to be used for TRA task.

Xavier et al. \cite{xavier:2013:RT} proposed a generic neural network architecture using Recurrent Neural Network based on reservoir computing approaches, namely Echo State Network to solve TRA task. The proposed architecture models the language acquisition in the brain and provided a robust and scalable implementation on robotics architecture \cite{xavier:2013:RT,tra:xavier_hri}. They called this model as $\theta RARes$. The model is based on the notion of grammatical construction: mapping of word order (surface form) to its meaning. They first transformed the raw sentences by replacing the semantic words (nouns, verbs etc.) with a unique token $'SW'$ then the sentences are input to model sequentially, word by word across time along with the coded meaning (i.e. thematic roles of semantic words) of the input sentence for training. The model learns the thematic roles of all the semantic words in the input sentence during training. During testing, the model predicts the coded meaning of the previously unseen sentences. See chapter \ref{issues} for more details about $\theta RARes$ model.

\section{Motivation and Hypothesis}

Like many other traditional NLP systems, they also treated words as discrete atomic symbols and used localist vector representation of words as an input. Treating each word as a discrete symbol does not provide any relational information to the model which may exist between two words. For example, if words \textit{'pink'} and \textit{'red'} are represented using localist representation with vectors [1,0] and [0,1] respectively, then the semantic relationship (i.e. both are colors) between these two words is lost \cite{w2v:tensor_flow}. Although, replacing the semantic words with $'SW'$ token makes it possible to train the model on a small corpus as the $'SW'$ token can be replaced with different semantic words (nouns, verbs etc.) to form a sentence. Whereas on the other hand, the 'SW' token in itself does not carry any semantics and thus does not allow the model to take into account the semantics of the words. This makes it difficult for the model to learn thematic roles for sentences. We discuss more in detail about the limitation of this model later in Chapter \ref{issues}.

Motivated by the limitations of localist representation of words and transformation of raw sentences into its abstract form by replacing semantic words with 'SW' token described above, we hypothesize that the use of distributed word representation which can capture the syntactic and semantic relationship of words could possibly improve the performance of the model on TRA task. One such model for learning distributed word vectors was proposed by Mikolov et al. \cite{w2v:mikolov_2013_distributed} widely known as the Word2Vec model. Word2Vec model learns high quality, low-dimensional vector representation of words from a large corpus in an unsupervised way. The resulting word vector of this model encodes semantics of words. As the model learns the embeddings by taking into account the context words, the obtained vector embeddings also encode several language regularities and patterns[ref] and can be observed by performing the linear operations on the word vector. For example, \textit{vector('king') - vector('man') + vector('woman') $\approx$ vector('queen')}. Unlike other neural network models for obtaining word embeddings, training Word2Vec is computationally cheap and efficient \cite{w2v:mikolov_2013_distributed}. Training of word2vec model and properties of resulting distributed word embeddings will be discussed later in more detail in Chapter \ref{basics}.  

\section{Proposed Models}

In this work, we thus propose an end-to-end system called \textit{Word2Vec-ESN} model, for TRA task. The Word2Vec-ESN model is a combination of word2vec model and ESN. The word2vec model is trained on a general purpose unlabeled dataset (e.g. Wikipedia) prior to use of the model for TRA task. The word2vec unit being the first unit receives the raw sentences and generates the distributed word embeddings of the constituent words. The generated word vector by word2vec model can then be used by ESN for learning thematic roles of the input sentences. Note that the proposed model is basically a modified version of $\theta RARes$ model \cite{xavier:2013:RT}, where unlike the latter raw sentences are not transformed to grammatical construction and word2vec word vectors are used over localist word representation as an input to ESN. 

Apart from Word2Vec-ESN model, we also propose a variant of this model which only differs from the original in the way the sentences are processed and results are evaluated. Thus in this model variant, the inputs and outputs of the model are changed. The input feature to this model variant is the current word and the verb with respect to which it is processed. The output units encode the possible role (e.g. predicate, agent, object, recipient and No Role) of the input words, unlike the original model where output units encode the thematic roles of all semantic words in the input sentence. For the evaluation of this model variant, we used metrics (classification scores) proposed for CoNLL-2005 SRL task \cite{conll:2005,conll:2004}. Both Word2Vec-ESN model and its variant are discussed in more detail in chapter \ref{approach}.

[TODO:Describe the overview of results here.]

\section{Scope of work}

There are several other ways of obtaining word embeddings [glove and other] but a systematic comparison of them on TRA task is beyond the scope of this work. Using word2vec model, distributed word vectors of different dimensions can be obtained and used for TRA task. Evaluating and comparing the effect of the dimension of  word embedding on the proposed model is also not the focus of this study. To this date, there is no research conducted with this combination of word2vec model and ESN. This also makes this study novel.

\section{Outline}

In the next chapter, we give a description of  word2vec model and echo state network model. We also describe in detail the training of word2vec model and properties of word2vec word vectors. Also, this chapter describes training and control parameters of ESN. The chapter \ref{issues} describes the  $\theta RARes$ model, its limitations and the motivation and hypothesis for the current work. Subsequently in chapter \ref{approach} we propose the Word2Vec-ESN model and its variant. This chapter also describes the implementation and the metrics used to evaluate the performance of the proposed model and its variant. Chapter \ref{results} contains the corpora used and the experimental setup. we also describe  the experiments  performed and report the results obtained for TRA task with proposed models . In this chapter, we also compare the results of Word2Vec-ESN model with the results obtained from  $\theta RARes$ model and analysis them. Finally, in the chapter \ref{conclusion} we describe the conclusion of this study and the possible future work.