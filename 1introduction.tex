\chapter{Introduction}\label{introduction}

Thematic Role Assignment (TRA) is a supervised learning problem to determine who did what to whom [ref?]. In other words assigning roles to words (arguments) in a sentence with respect to a verb (predicate). The role typically includes agent, object, recipient etc.. For example in the sentence "the dog that gave the rat to the cat was hit by the man", the first noun 'dog' is the agent of verb 'gave' and object of verb 'hit'. In Natural Language Processing terminology (NLP) the problem is studied under the name of Semantic Role Labelling (SRL). Hence TRA or SRL is a form of simplistic semantic parsing which aims to determine the predicate-argument structure for a verb in the given sentence \cite{end-to-end}.

Many successful traditional system consider SRL as a multiclass classification problem use linear classifier such as Support Vector Machines (SVM) to tackle the problem \cite{Koomen:2005,pradhan:2005}. These system were based on pre-defined feature templates derived from syntactic information obtained by parsing the training corpus. However in an analysis it was found that the use of syntatic parser certainly leads to degradation of predictions \cite{pradhan:2005}. Designing of feature templates need a lot of heuristics and time. The pre-defined features are often required to be iteratively modified depending on how the system perfoms. Also the feature templates are often required to be re-desinged when the task is to be performed on different languages, corpus or when the data distribution is changed \cite{end-to-end}.

Apart from the traditional methods, the task was also attempted with neural network models. [Collobert 2011] first attempted to build an end-to-end system without parsing using word embeddings and Convolutional Neural Network (CNN). The model was less successful as CNN cannot employs long term dependencies within a sentence since it can only take into account the words in limitied context[ref]. However to increase the model perfomance they also resorted to use syntatic features by using parse trees[ref].

Xavier et al., \cite{xavier:2013:RT} proposed a generic neural network architecture using Recurrent Neural Network (RNN) based on reservoir computing approaches, namely Echo State Networks (ESN) to solve TRA task. The choice of ESN over simple RNN has three major advantages. First, ESNs are capable of modeling long term dependecies in the sentence [ref].Second, while processing long sequence the gradiend parameter vanishes or explodes in simple RNNs. Third, unlike simple RNN ESNs are computationally cheap as in ESN the recurrent layer (reservoir) is randomly initialized and only connections from recurrent layer to read-out layers are learned[ref]. The proposed architecutre models the language acquisition in brain and provided a robust and scalable implementation on robotics architecture(hinaut 2013,2014). 

The model is based on the notion of grammatical construction: mapping of word order (surface form) to its meaning. They first transformed the raw sentences by replacing the semantic words (nouns, verbs etc.) with a unique token $'SW'$ and then the sentences are input to model sequentially, word by word across time. The output units coding the the role for each semantic word with respect to a verb is then learned by ESN while processing the sentence. Like other traditional NLP system they also treated words as discrete atomic symbols and used localist vector respresentaion as input for words. Treating each word as an dicrete symbol does not provide any realtional information to the model which exist between two words. For example if word pink and red is  represented using localist representation by vectors [1,0,0] and [0,0,1] respectively then the semantic relationship between these two words are lost. Although replacing of semantic words with $'SW'$ token makes easy to train the model on small corpus as the $'SW'$ token can be replaced with different semantic words to form a sentence. Whereas on the other hand this makes it difficult for the model to learn thematic roles for ambiguous sentences (sentences with same surface form but different coded meaning). We discuss more in detail about the limitation of this model later in Chapter \ref{issues}.

Motivated by the limitations of localist input representation of words and transformation of raw sentences to grammatical constructions described above, we hypothesis that the use of distributed word representation which can capture the semantic relation of words could possibly improve the perfomance of the model on TRA task. One such model for learning distributed word vectors was proposed by Mikolov et al. [ref] widely known as Word2Vec model. Word2Vec model learn high quality, low-dimensional vector represenation encoding semantic meaning of words from a large corpus in an unsupervised way[ref-two]. The obtained vector embeddings also encodes several language regularities and patterns[] and can be seen by performing linear operation on the vector. For example, vector('king')-vector('man')+vector('woman') $\approx$ vector('queen').  Unlike other neural network models for obtainig word embeddings, training Word2Vec is computationally cheap and efficient[ref?] . See Chapter \ref{basics} for training word2vec model. 

In this work, we thus propose to extend the xavier's model to obtain an end-to-end system by adding an additional word2vec unit trained on a general purpose unlabelled dataset(e.g. wikipedia) to generate word embeddings for the words of input sentences. The generated word vector by word2vec model can then be used by ESN for learning thematic roles of the input sentences. There are also several other ways of obtaining word embeddings[glove and other] but a systematic comparision of them on TRA task is beyond the scope of this work. To this date, there is no research conducted with this combination of word2vec model and ESN.

Apart from adding an additional word2vec unit in the existing model we also propose a variant of this model. In this model variant input and output coding for the model is changed. The input feautre to this model variant is the current word and the verb with respect to which it is processed. The output units encodes the possible role for the input word. For the evaluation of this model variant we used metrics (classification scores) proposed for CoNLL-2005 SRL task[ref]. See chapter \ref{approach} for more details.






