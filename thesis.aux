\relax 
\providecommand\hyper@newdestlabel[2]{}
\AC@reset@newl@bel
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\newlabel{sec:abstract}{{}{III}{Abstract}{section*.1}{}}
\newlabel{sec:zusammenfassung}{{}{III}{Zusammenfassung}{section*.2}{}}
\citation{xavier:2013:RT}
\citation{xavier:2013:RT}
\citation{end-to-end}
\citation{Koomen:2005}
\citation{pradhan:2005}
\citation{pradhan:2005}
\citation{end-to-end}
\citation{xavier:2013:RT}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{introduction}{{1}{1}{Introduction}{chapter.1}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Basics of Word2Vec and Echo State Network}{5}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{basics}{{2}{5}{Basics of Word2Vec and Echo State Network}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Word2Vec}{5}{section.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}CBOW Model}{5}{subsection.2.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces The CBOW model}}{6}{figure.caption.6}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:cbow}{{2.1}{6}{The CBOW model}{figure.caption.6}{}}
\newlabel{eqn:hidden_act}{{2.1.1}{6}{CBOW Model}{equation.2.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces The Skip-gram model}}{7}{figure.caption.7}}
\newlabel{fig:sg}{{2.2}{7}{The Skip-gram model}{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Skip-gram Model}{8}{subsection.2.1.2}}
\newlabel{eqn:sg_prob}{{2.1.8}{8}{Skip-gram Model}{equation.2.1.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{Skip-gram with negative sampling}{8}{section*.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Properties of Word2Vec embeddings}{9}{subsection.2.1.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Word2vec semantic regularities}}{9}{figure.caption.9}}
\newlabel{fig:sem_rel}{{2.3}{9}{Word2vec semantic regularities}{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Word2Vec language translation property}}{9}{figure.caption.10}}
\newlabel{fig:w2v_translation}{{2.4}{9}{Word2Vec language translation property}{figure.caption.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Echo State Network (ESN)}{10}{section.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}ESN Architecture}{10}{subsection.2.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces \textbf  {Architecture of classical ESN:} The reservoir is the recurrent neural network with N units and initialized with random connection. The reservoir is provided input $u(n)$ to the input layer and teacher layer $y(n)$ are pushed output neurons respectively during training. The input to reservoir weights ($W^{in}$), output neurons to reservoir ($W^{back}$, optional and depends on task) and reservoir to reservoir weight ($W^{res}$) from are also randomly initialized and stays static during learning. The output weight from reservoir to output unit are the only weights learned by the network during trainging.[ESN9]\relax }}{11}{figure.caption.11}}
\newlabel{fig:esn_arch}{{2.5}{11}{\textbf {Architecture of classical ESN:} The reservoir is the recurrent neural network with N units and initialized with random connection. The reservoir is provided input $u(n)$ to the input layer and teacher layer $y(n)$ are pushed output neurons respectively during training. The input to reservoir weights ($W^{in}$), output neurons to reservoir ($W^{back}$, optional and depends on task) and reservoir to reservoir weight ($W^{res}$) from are also randomly initialized and stays static during learning. The output weight from reservoir to output unit are the only weights learned by the network during trainging.[ESN9]\relax }{figure.caption.11}{}}
\newlabel{eqn:res_scaling}{{2.2.1}{11}{ESN Architecture}{equation.2.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Training ESN}{11}{subsection.2.2.2}}
\newlabel{ssec:esn_training}{{2.2.2}{11}{Training ESN}{subsection.2.2.2}{}}
\newlabel{eqn:rmse}{{2.2.2}{12}{Training ESN}{equation.2.2.2}{}}
\newlabel{eqn:res_update}{{2.2.3}{12}{Training ESN}{equation.2.2.3}{}}
\newlabel{eqn:res_state}{{2.2.4}{12}{Training ESN}{equation.2.2.4}{}}
\newlabel{eqn:esn_output}{{2.2.5}{12}{Training ESN}{equation.2.2.5}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Previous Work and Open Issues}{15}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{issues}{{3}{15}{Previous Work and Open Issues}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Grammatical Construction and Thematic Role Assingment}{15}{section.3.1}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Transformation of a sentence by replacing semantic words with 'SW' token and localist vector representation of words used as an input for a sentence.\relax }}{16}{table.caption.12}}
\newlabel{tab:localist_representation}{{3.1}{16}{Transformation of a sentence by replacing semantic words with 'SW' token and localist vector representation of words used as an input for a sentence.\relax }{table.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Limitation of using grammatical construction}{16}{subsection.3.1.1}}
\@writefile{toc}{\contentsline {paragraph}{Example 1: Ambiguous Sentences}{16}{section*.13}}
\newlabel{eg-1:sent-1}{{1}{16}{Example 1: Ambiguous Sentences}{Item.1}{}}
\newlabel{eg-1:sent-2}{{2}{16}{Example 1: Ambiguous Sentences}{Item.2}{}}
\newlabel{eg-1:sent-3}{{3}{16}{Example 1: Ambiguous Sentences}{Item.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Training with Sentence \ref  {eg-1:sent-1}:}{16}{section*.14}}
\@writefile{toc}{\contentsline {paragraph}{Training with Sentence \ref  {eg-1:sent-2}:}{17}{section*.16}}
\@writefile{toc}{\contentsline {paragraph}{Testing with Sentence \ref  {eg-1:sent-3}:}{17}{section*.18}}
\newlabel{eg-1:sent-i}{{{{(i)}}}{17}{Testing with Sentence \ref {eg-1:sent-3}:}{Item.4}{}}
\newlabel{eg-1:sent-ii}{{{{(ii)}}}{17}{Testing with Sentence \ref {eg-1:sent-3}:}{Item.5}{}}
\newlabel{eg-1:sent-iii}{{{{(iii)}}}{18}{Testing with Sentence \ref {eg-1:sent-3}:}{Item.6}{}}
\@writefile{toc}{\contentsline {paragraph}{Example 2: Ambiguous and Polysemous Sentences}{18}{section*.20}}
\newlabel{eg-2:sent-1}{{1}{18}{Example 2: Ambiguous and Polysemous Sentences}{Item.7}{}}
\newlabel{eg-2:sent-2}{{2}{18}{Example 2: Ambiguous and Polysemous Sentences}{Item.8}{}}
\@writefile{toc}{\contentsline {paragraph}{Training ESN on sentence \ref  {eg-2:sent-1}}{18}{section*.21}}
\@writefile{toc}{\contentsline {paragraph}{Testing ESN on sentence \ref  {eg-2:sent-2}}{18}{section*.22}}
\@writefile{toc}{\contentsline {paragraph}{Issue with Polysemous words:}{18}{section*.23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Research Hypothesis}{18}{subsection.3.1.2}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Approaching Word2Vec and ESN Language Model}{21}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{approach}{{4}{21}{Approaching Word2Vec and ESN Language Model}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Proposed Model}{21}{section.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces \textbf  {Architecture of Word2Vec-ESN model:} The model takes the words of a sentence as an input across time. Word2Vec model generates the distributed vector representation of the input word. The generated word vector is then used by ESN for further processing and learns to predict the thematic roles of the input sentence.\relax }}{21}{figure.caption.24}}
\newlabel{fig:model_arch}{{4.1}{21}{\textbf {Architecture of Word2Vec-ESN model:} The model takes the words of a sentence as an input across time. Word2Vec model generates the distributed vector representation of the input word. The generated word vector is then used by ESN for further processing and learns to predict the thematic roles of the input sentence.\relax }{figure.caption.24}{}}
\citation{xavier:2013:RT}
\citation{xavier:2013:RT}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Model Variant-1}{22}{subsection.4.1.1}}
\citation{xavier:2013:RT}
\citation{xavier:2013:RT}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces \textbf  {Word2Vec-ESN Model Variant-1:} The figure shows the processing of a sentence by the model variant at time step 1. Nouns and verbs (specified in orange and green respectively) are stored in a memory stack for interpretating coded meaning. The word 'John' is input to word2vec model which generate a word vector of $E_{v}$ dimensions. The output vector is then input to ESN for further processing. During training the readout units are presented with the coded meaning of the input sentence(i.e. N1-A1: Noun-1 is Agent of verb 1, N2-O1: Noun-2 is object of verb 2). In testing, the readout units codes the predicted meaning of input sentence. The meaning: hit(John,ball,--) is decoded by mapping the thematic roles predicted by readout neurons with nouns and verbs from memory stack. Adapted from \cite  {xavier:2013:RT}\relax }}{23}{figure.caption.25}}
\newlabel{fig:model_variant_1}{{4.2}{23}{\textbf {Word2Vec-ESN Model Variant-1:} The figure shows the processing of a sentence by the model variant at time step 1. Nouns and verbs (specified in orange and green respectively) are stored in a memory stack for interpretating coded meaning. The word 'John' is input to word2vec model which generate a word vector of $E_{v}$ dimensions. The output vector is then input to ESN for further processing. During training the readout units are presented with the coded meaning of the input sentence(i.e. N1-A1: Noun-1 is Agent of verb 1, N2-O1: Noun-2 is object of verb 2). In testing, the readout units codes the predicted meaning of input sentence. The meaning: hit(John,ball,--) is decoded by mapping the thematic roles predicted by readout neurons with nouns and verbs from memory stack. Adapted from \cite {xavier:2013:RT}\relax }{figure.caption.25}{}}
\newlabel{eg:SCL}{{1}{23}{Model Variant-1}{Item.9}{}}
\citation{xavier:2013:RT}
\citation{xavier:2013:RT}
\citation{xavier:2013:RT}
\citation{xavier:2013:RT}
\citation{xavier:2013:RT}
\newlabel{eg:SFL}{{2}{24}{Model Variant-1}{Item.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{Decoding Output}{24}{section*.26}}
\@writefile{toc}{\contentsline {subsubsection}{Evaluation Metrics}{24}{section*.27}}
\newlabel{ssec:evaluation_mertics_1}{{4.1.1}{24}{Evaluation Metrics}{section*.27}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Model Variant-2}{24}{subsection.4.1.2}}
\citation{xavier:2013:RT}
\citation{xavier:2013:RT}
\citation{confusion_martrix:1998}
\citation{macro_average:2005}
\citation{conll:2004}
\citation{conll:2005}
\newlabel{tab:argument-predicate}{{\caption@xref {tab:argument-predicate}{ on input line 62}}{25}{Model Variant-2}{table.caption.28}{}}
\@writefile{toc}{\contentsline {subsubsection}{Decoding Output}{25}{section*.30}}
\@writefile{toc}{\contentsline {subsubsection}{Evaluation Metrics}{25}{section*.31}}
\newlabel{ssec:evaluation_metrics_2}{{4.1.2}{25}{Evaluation Metrics}{section*.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces \textbf  {Word2Vec-ESN Model Variant-2:} The figure shows the process of a sentence in model variant-2 at time step 1. At any instant of time an argument (current word, marked in orange) and predicate (verb,marked in green) is input to the model. Word2Vec model generates the word vectors of $E_{v}$ dimensions which are then cocatenated to form a $2 \times E_{v}$ dimensions(shown in orange and green color). ESN takes the resultant vector for further processing. During learning, the readout neurons are presented with the role of input word (i.e. A (Agent)). The read-out weights (shown in dashed line) are learned during training. During testing the readout unit codes the role of input words, which are then accumulated and decoded to meaning \textit  {hit(John,ball,--)} at the end of sentence. Inspired from \cite  {xavier:2013:RT} \relax }}{26}{figure.caption.29}}
\newlabel{fig:model_variant_2}{{4.3}{26}{\textbf {Word2Vec-ESN Model Variant-2:} The figure shows the process of a sentence in model variant-2 at time step 1. At any instant of time an argument (current word, marked in orange) and predicate (verb,marked in green) is input to the model. Word2Vec model generates the word vectors of $E_{v}$ dimensions which are then cocatenated to form a $2 \times E_{v}$ dimensions(shown in orange and green color). ESN takes the resultant vector for further processing. During learning, the readout neurons are presented with the role of input word (i.e. A (Agent)). The read-out weights (shown in dashed line) are learned during training. During testing the readout unit codes the role of input words, which are then accumulated and decoded to meaning \textit {hit(John,ball,--)} at the end of sentence. Inspired from \cite {xavier:2013:RT} \relax }{figure.caption.29}{}}
\citation{classification_scores:2009}
\citation{accuracy_paradox_1:2008}
\citation{accuracy_paradox_2:2014}
\citation{classification_scores:2009}
\citation{classification_scores:2009}
\citation{classification_scores:2009}
\newlabel{eqn:accuracy}{{4.1.1}{27}{Evaluation Metrics}{equation.4.1.1}{}}
\newlabel{eqn:precision}{{4.1.2}{27}{Evaluation Metrics}{equation.4.1.2}{}}
\newlabel{eqn:recall}{{4.1.3}{27}{Evaluation Metrics}{equation.4.1.3}{}}
\newlabel{eqn:precision}{{4.1.4}{27}{Evaluation Metrics}{equation.4.1.4}{}}
\citation{xavier:2013:RT}
\citation{xavier:2013:RT}
\citation{xavier:2013:RT}
\citation{xavier:2013:RT}
\citation{xavier:2013:RT}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Dataset and pre-processing}{28}{section.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Corpus For TRA Task}{28}{subsection.4.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Corpus For Training Word2Vec Model}{29}{subsection.4.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Obtaining Word Embeddings}{29}{section.4.3}}
\newlabel{get_word_embeddings}{{4.3}{29}{Obtaining Word Embeddings}{section.4.3}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Experiments and Results}{31}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{results}{{5}{31}{Experiments and Results}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Input and Ouput Coding}{31}{section.5.1}}
\@writefile{toc}{\contentsline {paragraph}{Model Variant-1:}{31}{section*.32}}
\@writefile{toc}{\contentsline {paragraph}{Model Variant-2:}{31}{section*.33}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Experiments}{32}{section.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Experiment-1: Learning thematic roles}{32}{subsection.5.2.1}}
\@writefile{toc}{\contentsline {paragraph}{Model Variant-1:}{32}{section*.34}}
\@writefile{toc}{\contentsline {paragraph}{Model Variant-2:}{32}{section*.35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Experiment-2: Generalization Capabilities}{32}{subsection.5.2.2}}
\newlabel{exp-2}{{5.2.2}{32}{Experiment-2: Generalization Capabilities}{subsection.5.2.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Model Variant-1:}{33}{section*.36}}
\@writefile{toc}{\contentsline {paragraph}{Model Variant-2:}{33}{section*.37}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Training and testing classification scores for individual roles when using model variant-2 with word2vec vector embeddings of words for input sentences.\relax }}{34}{table.caption.39}}
\newlabel{tab:classsification-scores-21}{{5.1}{34}{Training and testing classification scores for individual roles when using model variant-2 with word2vec vector embeddings of words for input sentences.\relax }{table.caption.39}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}Experiment-3: Effect of Corpus structure}{34}{subsection.5.2.3}}
\citation{xavier:2013:RT}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Training and testing classification scores for individual roles when using input sentences in GC form with localist word vector.\relax }}{35}{table.caption.41}}
\newlabel{tab:classsification-scores-22}{{5.2}{35}{Training and testing classification scores for individual roles when using input sentences in GC form with localist word vector.\relax }{table.caption.41}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.4}Experiment-4: Effect of Reservoir size}{35}{subsection.5.2.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces \textbf  {Normalized Confusion matrix when using word2vec word embeddings:} The confusion matrix with true roles (in rows) and predicted roles (in columns). The top-left to bottom-right diagonal shows the percentage of values predicted correctly.Everything other than this diagonal represents the incorrect prediction of roles. Model identified almost all words labelled as Recipient , Verb and No Role and made some error in predicting Agent and Object. The results were obtained with reservoir of 1000 neurons and 10 fold-cross validation.\relax }}{36}{figure.caption.42}}
\newlabel{fig:confusion_matrix}{{5.1}{36}{\textbf {Normalized Confusion matrix when using word2vec word embeddings:} The confusion matrix with true roles (in rows) and predicted roles (in columns). The top-left to bottom-right diagonal shows the percentage of values predicted correctly.Everything other than this diagonal represents the incorrect prediction of roles. Model identified almost all words labelled as Recipient , Verb and No Role and made some error in predicting Agent and Object. The results were obtained with reservoir of 1000 neurons and 10 fold-cross validation.\relax }{figure.caption.42}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces Mean and standard deviation of meaning and sentence error on train and test set of coprus-462 in different learning modes.\relax }}{37}{table.caption.44}}
\newlabel{tab:corpus-structure}{{5.3}{37}{Mean and standard deviation of meaning and sentence error on train and test set of coprus-462 in different learning modes.\relax }{table.caption.44}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces \textbf  {Effect of reservoir size on cross validation errors on Model Variant-1:} Description goes here.\relax }}{37}{figure.caption.45}}
\newlabel{fig:reservoir_size_1}{{5.2}{37}{\textbf {Effect of reservoir size on cross validation errors on Model Variant-1:} Description goes here.\relax }{figure.caption.45}{}}
\citation{end-to-end}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces \textbf  {Effect of reservoir size on classification scores of Model Varinat-2:} Description goes here.\relax }}{38}{figure.caption.46}}
\newlabel{fig:reservoir_size_2}{{5.3}{38}{\textbf {Effect of reservoir size on classification scores of Model Varinat-2:} Description goes here.\relax }{figure.caption.46}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.5}Experiment-5: Effect of Corpus size}{38}{subsection.5.2.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces \textbf  {Effect of corpus size on cross validation errors:} Description goes here.\relax }}{39}{figure.caption.47}}
\newlabel{fig:corpus_size_1}{{5.4}{39}{\textbf {Effect of corpus size on cross validation errors:} Description goes here.\relax }{figure.caption.47}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces \textbf  {Effect of corpus size on cross validation errors using localist word vector as reported in [ref?]:} Description goes here.\relax }}{39}{figure.caption.48}}
\newlabel{fig:corpus_size_xavier}{{5.5}{39}{\textbf {Effect of corpus size on cross validation errors using localist word vector as reported in [ref?]:} Description goes here.\relax }{figure.caption.48}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.6}Experiment-6: Online re-analysis of sentences}{40}{subsection.5.2.6}}
\newlabel{eg-1:or-sent-17}{{1}{40}{Experiment-6: Online re-analysis of sentences}{Item.13}{}}
\newlabel{eg-1:or-sent-23}{{2}{40}{Experiment-6: Online re-analysis of sentences}{Item.14}{}}
\newlabel{eg-1:or-sent-27}{{3}{40}{Experiment-6: Online re-analysis of sentences}{Item.15}{}}
\newlabel{eg-1:or-sent-16}{{4}{40}{Experiment-6: Online re-analysis of sentences}{Item.16}{}}
\newlabel{eg-1:or-sent-17}{{1}{40}{Experiment-6: Online re-analysis of sentences}{Item.17}{}}
\newlabel{eg-1:or-sent-23}{{2}{40}{Experiment-6: Online re-analysis of sentences}{Item.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces \textbf  {Effect of corpus size on cross validation errors using localist word vector as reported in [ref?]:} Description goes here.\relax }}{41}{figure.caption.49}}
\newlabel{fig:act_analysis_1}{{5.6}{41}{\textbf {Effect of corpus size on cross validation errors using localist word vector as reported in [ref?]:} Description goes here.\relax }{figure.caption.49}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces \textbf  {Effect of corpus size on cross validation errors using localist word vector as reported in [ref?]:} Description goes here.\relax }}{41}{figure.caption.50}}
\newlabel{fig:act_analysis_2}{{5.7}{41}{\textbf {Effect of corpus size on cross validation errors using localist word vector as reported in [ref?]:} Description goes here.\relax }{figure.caption.50}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces \textbf  {Effect of corpus size on cross validation errors using localist word vector as reported in [ref?]:} Description goes here.\relax }}{42}{figure.caption.51}}
\newlabel{fig:act_analysis_3}{{5.8}{42}{\textbf {Effect of corpus size on cross validation errors using localist word vector as reported in [ref?]:} Description goes here.\relax }{figure.caption.51}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusion And Future Work}{43}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{conclusion}{{6}{43}{Conclusion And Future Work}{chapter.6}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Nomenclature}{45}{appendix.A}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{app:nomenclature}{{A}{45}{Nomenclature}{appendix.A}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Additional Proofs}{47}{appendix.B}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{app:proofs}{{B}{47}{Additional Proofs}{appendix.B}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {C}Complete Simulation Results}{49}{appendix.C}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{app:completeResults}{{C}{49}{Complete Simulation Results}{appendix.C}{}}
\bibstyle{plain}
\bibdata{thesis,esn,word2vec}
\bibcite{confusion_martrix:1998}{1}
\bibcite{conll:2005}{2}
\bibcite{conll:2004}{3}
\bibcite{xavier:2013:RT}{4}
\bibcite{Koomen:2005}{5}
\bibcite{macro_average:2005}{6}
\bibcite{pradhan:2005}{7}
\bibcite{classification_scores:2009}{8}
\bibcite{accuracy_paradox_1:2008}{9}
\bibcite{accuracy_paradox_2:2014}{10}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{51}{appendix.C}}
\bibcite{end-to-end}{11}
\newlabel{sec:urheber}{{C}{53}{Erkl\"arung der Urheberschaft}{appendix*.53}{}}
\newlabel{sec:urheber}{{C}{55}{Erkl\"arung zur Ver\"offentlichung}{appendix*.54}{}}
