\relax 
\providecommand\hyper@newdestlabel[2]{}
\AC@reset@newl@bel
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\newlabel{sec:abstract}{{}{III}{Abstract}{section*.1}{}}
\newlabel{sec:zusammenfassung}{{}{III}{Zusammenfassung}{section*.2}{}}
\citation{xavier:2013:RT}
\citation{xavier:2013:RT}
\citation{end-to-end}
\citation{Koomen:2005}
\citation{pradhan:2005}
\citation{pradhan:2005}
\citation{end-to-end}
\citation{xavier:2013:RT}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{introduction}{{1}{1}{Introduction}{chapter.1}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Basics of Word2Vec and Echo State Network}{5}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{basics}{{2}{5}{Basics of Word2Vec and Echo State Network}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Word2Vec}{5}{section.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}CBOW Model}{5}{subsection.2.1.1}}
\newlabel{eqn:hidden_act}{{2.1.1}{6}{CBOW Model}{equation.2.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Skip-gram Model}{7}{subsection.2.1.2}}
\@writefile{toc}{\contentsline {subsubsection}{Skip-gram with negative sampling}{7}{section*.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Properties of Word2Vec embeddings}{7}{subsection.2.1.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces \textbf  {Word2vec Semantic regularities:} Description goes here.\relax }}{8}{figure.caption.7}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:sem_rel}{{2.1}{8}{\textbf {Word2vec Semantic regularities:} Description goes here.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces \textbf  {Word2Vec Translation Property:} Description goes here [ESN9]\relax }}{8}{figure.caption.8}}
\newlabel{fig:w2v_translation}{{2.2}{8}{\textbf {Word2Vec Translation Property:} Description goes here [ESN9]\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Echo State Network (ESN)}{9}{section.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}ESN Architecture}{9}{subsection.2.2.1}}
\newlabel{eqn:res_scaling}{{2.2.1}{9}{ESN Architecture}{equation.2.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces \textbf  {Architecture of classical ESN:} The reservoir is the recurrent neural network with N units and initialized with random connection. The reservoir is provided input from the input u(n) and y(n) from the input and output neurons respectively. The input weights Win from input and Wback from output neurons are also randomly initialized and stays static during learning. The output weight from reservoir to output unit is learned by the network.[ESN9]\relax }}{10}{figure.caption.9}}
\newlabel{fig:esn_arch}{{2.3}{10}{\textbf {Architecture of classical ESN:} The reservoir is the recurrent neural network with N units and initialized with random connection. The reservoir is provided input from the input u(n) and y(n) from the input and output neurons respectively. The input weights Win from input and Wback from output neurons are also randomly initialized and stays static during learning. The output weight from reservoir to output unit is learned by the network.[ESN9]\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Training ESN}{10}{subsection.2.2.2}}
\newlabel{ssec:esn_training}{{2.2.2}{10}{Training ESN}{subsection.2.2.2}{}}
\newlabel{eqn:rmse}{{2.2.2}{10}{Training ESN}{equation.2.2.2}{}}
\newlabel{eqn:res_update}{{2.2.3}{11}{Training ESN}{equation.2.2.3}{}}
\newlabel{eqn:res_state}{{2.2.4}{11}{Training ESN}{equation.2.2.4}{}}
\newlabel{eqn:esn_output}{{2.2.5}{11}{Training ESN}{equation.2.2.5}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Previous Work and Open Issues}{13}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{issues}{{3}{13}{Previous Work and Open Issues}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Grammatical Construction and Thematic Role Assingment}{13}{section.3.1}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Table Caption\relax }}{14}{table.caption.10}}
\newlabel{tab:gc}{{3.1}{14}{Table Caption\relax }{table.caption.10}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces My caption\relax }}{14}{table.caption.11}}
\newlabel{tab:localist_representation}{{3.2}{14}{My caption\relax }{table.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Limitation of using grammatical construction}{14}{subsection.3.1.1}}
\newlabel{eg-1:sent-1}{{1}{14}{Limitation of using grammatical construction}{Item.1}{}}
\newlabel{eg-1:sent-2}{{2}{14}{Limitation of using grammatical construction}{Item.2}{}}
\newlabel{eg-1:sent-3}{{3}{14}{Limitation of using grammatical construction}{Item.3}{}}
\newlabel{eg-1:sent-i}{{{{(i)}}}{16}{Limitation of using grammatical construction}{Item.4}{}}
\newlabel{eg-1:sent-ii}{{{{(ii)}}}{16}{Limitation of using grammatical construction}{Item.5}{}}
\newlabel{eg-1:sent-iii}{{{{(iii)}}}{16}{Limitation of using grammatical construction}{Item.6}{}}
\newlabel{eg-2:sent-1}{{1}{16}{Limitation of using grammatical construction}{Item.7}{}}
\newlabel{eg-2:sent-2}{{2}{16}{Limitation of using grammatical construction}{Item.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Research Hypothesis}{16}{subsection.3.1.2}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Approaching Word2Vec and ESN Language Model}{19}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{approach}{{4}{19}{Approaching Word2Vec and ESN Language Model}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Proposed Model}{19}{section.4.1}}
\citation{xavier:2013:RT}
\citation{xavier:2013:RT}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces \textbf  {Architecture of classical ESN:} The reservoir is the recurrent neural network with N units and initialized with random connection. The reservoir is provided input from the input u(n) and y(n) from the input and output neurons respectively. The input weights Win from input and Wback from output neurons are also randomly initialized and stays static during learning. The output weight from reservoir to output unit is learned by the network.[ESN9]\relax }}{20}{figure.caption.15}}
\newlabel{fig:esn_arch}{{4.1}{20}{\textbf {Architecture of classical ESN:} The reservoir is the recurrent neural network with N units and initialized with random connection. The reservoir is provided input from the input u(n) and y(n) from the input and output neurons respectively. The input weights Win from input and Wback from output neurons are also randomly initialized and stays static during learning. The output weight from reservoir to output unit is learned by the network.[ESN9]\relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Model Variant-1}{20}{subsection.4.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces \textbf  {Word2Vec-ESN Model Variant-1:} The figure shows the processing of a sentence by the model variant at time step 1. Nouns and verbs (specified in orange and green respectively) are stored in a memory stack for interpretating coded meaning. The word 'John' is input to word2vec model which generate a word vector of $E_{v}$ dimensions. The output vector is then input to ESN for further processing. During training the readout units are presented with the coded meaning of the input sentence(i.e. N1-A1: Noun-1 is Agent of verb 1, N2-O1: Noun-2 is object of verb 2). In testing, the readout units codes the predicted meaning of input sentence. The meaning: hit(John,ball,--) is decoded by mapping the thematic roles predicted by readout neurons with nouns and verbs from memory stack. Adapted from \cite  {xavier:2013:RT}\relax }}{21}{figure.caption.16}}
\newlabel{fig:model_variant_1}{{4.2}{21}{\textbf {Word2Vec-ESN Model Variant-1:} The figure shows the processing of a sentence by the model variant at time step 1. Nouns and verbs (specified in orange and green respectively) are stored in a memory stack for interpretating coded meaning. The word 'John' is input to word2vec model which generate a word vector of $E_{v}$ dimensions. The output vector is then input to ESN for further processing. During training the readout units are presented with the coded meaning of the input sentence(i.e. N1-A1: Noun-1 is Agent of verb 1, N2-O1: Noun-2 is object of verb 2). In testing, the readout units codes the predicted meaning of input sentence. The meaning: hit(John,ball,--) is decoded by mapping the thematic roles predicted by readout neurons with nouns and verbs from memory stack. Adapted from \cite {xavier:2013:RT}\relax }{figure.caption.16}{}}
\newlabel{eg:SCL}{{1}{21}{Model Variant-1}{Item.9}{}}
\newlabel{eg:SFL}{{2}{22}{Model Variant-1}{Item.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{Decoding Output}{22}{section*.17}}
\@writefile{toc}{\contentsline {subsubsection}{Evaluation Metrics}{22}{section*.18}}
\citation{xavier:2013:RT}
\citation{xavier:2013:RT}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Model Variant-2}{23}{subsection.4.1.2}}
\newlabel{tab:argument-predicate}{{\caption@xref {tab:argument-predicate}{ on input line 62}}{23}{Model Variant-2}{table.caption.19}{}}
\@writefile{toc}{\contentsline {subsubsection}{Decoding Output}{23}{section*.21}}
\@writefile{toc}{\contentsline {subsubsection}{Evaluation Metrics}{23}{section*.22}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces \textbf  {Word2Vec-ESN Model Variant-2:} The figure shows the process of a sentence in model variant-2 at time step 1. At any instant of time an argument (current word, marked in orange) and predicate (verb,marked in green) is input to the model. Word2Vec model generates the word vectors of $E_{v}$ dimensions which are then cocatenated to form a $2 \times E_{v}$ dimensions(shown in orange and green color). ESN takes the resultant vector for further processing. During learning, the readout neurons are presented with the role of input word (i.e. A (Agent)). The read-out weights (shown in dashed line) are learned during training. During testing the readout unit codes the role of input words, which are then accumulated and decoded to meaning \textit  {hit(John,ball,--)} at the end of sentence. Inspired from \cite  {xavier:2013:RT} \relax }}{24}{figure.caption.20}}
\newlabel{fig:model_variant_2}{{4.3}{24}{\textbf {Word2Vec-ESN Model Variant-2:} The figure shows the process of a sentence in model variant-2 at time step 1. At any instant of time an argument (current word, marked in orange) and predicate (verb,marked in green) is input to the model. Word2Vec model generates the word vectors of $E_{v}$ dimensions which are then cocatenated to form a $2 \times E_{v}$ dimensions(shown in orange and green color). ESN takes the resultant vector for further processing. During learning, the readout neurons are presented with the role of input word (i.e. A (Agent)). The read-out weights (shown in dashed line) are learned during training. During testing the readout unit codes the role of input words, which are then accumulated and decoded to meaning \textit {hit(John,ball,--)} at the end of sentence. Inspired from \cite {xavier:2013:RT} \relax }{figure.caption.20}{}}
\newlabel{eqn:accuracy}{{4.1.1}{24}{Evaluation Metrics}{equation.4.1.1}{}}
\newlabel{eqn:precision}{{4.1.2}{25}{Evaluation Metrics}{equation.4.1.2}{}}
\newlabel{eqn:precision}{{4.1.3}{25}{Evaluation Metrics}{equation.4.1.3}{}}
\newlabel{eqn:precision}{{4.1.4}{25}{Evaluation Metrics}{equation.4.1.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Comparision of Model Variants}{25}{section.4.2}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Dataset and pre-processing}{25}{section.4.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Corpus For TRA Task}{25}{subsection.4.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Corpus For Training Word2Vec Model}{26}{subsection.4.3.2}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Word Embeddings}{27}{section.4.4}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Experiments and Results}{29}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{results}{{5}{29}{Experiments and Results}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Input and Ouput Coding}{29}{section.5.1}}
\@writefile{toc}{\contentsline {paragraph}{Model Variant-1:}{29}{section*.23}}
\@writefile{toc}{\contentsline {paragraph}{Model Variant-2:}{29}{section*.24}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Experiments}{30}{section.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Experiment-1: Learning thematic roles}{30}{subsection.5.2.1}}
\@writefile{toc}{\contentsline {paragraph}{Model Variant-1:}{30}{section*.25}}
\@writefile{toc}{\contentsline {paragraph}{Model Variant-2:}{30}{section*.26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Experiment-2: Generalization Capabilities}{30}{subsection.5.2.2}}
\newlabel{exp-2}{{5.2.2}{30}{Experiment-2: Generalization Capabilities}{subsection.5.2.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Model Variant-1:}{31}{section*.27}}
\@writefile{toc}{\contentsline {paragraph}{Model Variant-2:}{31}{section*.28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}Experiment-3: Effect of Corpus structure}{31}{subsection.5.2.3}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Training and testing classification scores for individual roles\relax }}{32}{table.caption.30}}
\newlabel{tab:classsification-scores}{{5.1}{32}{Training and testing classification scores for individual roles\relax }{table.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces \textbf  {Normalized Confusion matrix:} Description goes here.\relax }}{32}{figure.caption.31}}
\newlabel{fig:confusion_matrix}{{5.1}{32}{\textbf {Normalized Confusion matrix:} Description goes here.\relax }{figure.caption.31}{}}
\citation{xavier:2013:RT}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Mean and standard deviation of meaning and sentence error on train and test set of coprus-462 in different learning modes.\relax }}{33}{table.caption.33}}
\newlabel{tab:corpus-structure}{{5.2}{33}{Mean and standard deviation of meaning and sentence error on train and test set of coprus-462 in different learning modes.\relax }{table.caption.33}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.4}Experiment-4: Effect of Reservoir size}{34}{subsection.5.2.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces \textbf  {Effect of reservoir size on cross validation errors on Model Varinat-1:} Description goes here.\relax }}{34}{figure.caption.34}}
\newlabel{fig:sem_rel}{{5.2}{34}{\textbf {Effect of reservoir size on cross validation errors on Model Varinat-1:} Description goes here.\relax }{figure.caption.34}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.5}Experiment-5: Effect of Corpus size}{34}{subsection.5.2.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces \textbf  {Effect of reservoir size on cross validation errors on Model Variant-2:} Description goes here.\relax }}{35}{figure.caption.35}}
\newlabel{fig:sem_rel}{{5.3}{35}{\textbf {Effect of reservoir size on cross validation errors on Model Variant-2:} Description goes here.\relax }{figure.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces \textbf  {Effect of corpus size on cross validation errors:} Description goes here.\relax }}{35}{figure.caption.36}}
\newlabel{fig:sem_rel}{{5.4}{35}{\textbf {Effect of corpus size on cross validation errors:} Description goes here.\relax }{figure.caption.36}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.6}Experiment-6: Online reanalysis of sentences}{36}{subsection.5.2.6}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusion And Future Work}{37}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{conclusion}{{6}{37}{Conclusion And Future Work}{chapter.6}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Nomenclature}{39}{appendix.A}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{app:nomenclature}{{A}{39}{Nomenclature}{appendix.A}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Additional Proofs}{41}{appendix.B}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{app:proofs}{{B}{41}{Additional Proofs}{appendix.B}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {C}Complete Simulation Results}{43}{appendix.C}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{app:completeResults}{{C}{43}{Complete Simulation Results}{appendix.C}{}}
\bibstyle{plain}
\bibdata{thesis}
\bibcite{xavier:2013:RT}{1}
\bibcite{Koomen:2005}{2}
\bibcite{pradhan:2005}{3}
\bibcite{end-to-end}{4}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{45}{appendix.C}}
\newlabel{sec:urheber}{{C}{47}{Erkl\"arung der Urheberschaft}{appendix*.38}{}}
\newlabel{sec:urheber}{{C}{49}{Erkl\"arung zur Ver\"offentlichung}{appendix*.39}{}}
