\relax 
\providecommand\hyper@newdestlabel[2]{}
\AC@reset@newl@bel
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\newlabel{sec:abstract}{{}{III}{Abstract}{section*.1}{}}
\newlabel{sec:zusammenfassung}{{}{III}{Zusammenfassung}{section*.2}{}}
\citation{end-to-end}
\citation{srl:machine_translation}
\citation{srl:info_extraction:hri}
\citation{srl:sentiment:wang}
\citation{srl:text_categorization:persson}
\citation{tra:xavier_hri}
\citation{srl:info_extraction:hri}
\citation{Koomen:2005}
\citation{srl:pradhan:2004}
\citation{pradhan:2005}
\citation{pradhan:2005}
\citation{end-to-end}
\citation{srl:collobert:2011}
\citation{end-to-end}
\citation{charniak_parser:2000}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{introduction}{{1}{1}{Introduction}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Previous Work}{1}{section.1.1}}
\citation{esn:learn_gs}
\citation{rnn:elman:1990}
\citation{rnn:gradiant_problem:bengio}
\citation{esn:NIPS:2003}
\citation{esn:practical_guide}
\citation{xavier:2013:RT}
\citation{xavier:2013:RT}
\citation{tra:xavier_hri}
\citation{w2v:tensor_flow}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Motivation and Hypothesis}{2}{section.1.2}}
\citation{w2v:mikolov_2013_distributed}
\citation{xavier:2013:RT}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Proposed Models}{3}{section.1.3}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Scope of work}{4}{section.1.4}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Outline}{4}{section.1.5}}
\citation{w2v:tensor_flow}
\citation{w2v:mikolov_2013_distributed}
\citation{w2v:baroni:2014}
\citation{w2v:mikolov_2013_efficient}
\citation{w2v:mikolov_2013_efficient}
\citation{w2v:mikolov_2013_distributed}
\citation{w2v:mikolov_2013_distributed}
\citation{w2v:parameter_learning}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Basics of Word2Vec and Echo State Network}{5}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{basics}{{2}{5}{Basics of Word2Vec and Echo State Network}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Word2Vec}{5}{section.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}CBOW Model}{5}{subsection.2.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces The CBOW model}}{6}{figure.caption.6}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:cbow}{{2.1}{6}{The CBOW model}{figure.caption.6}{}}
\newlabel{eqn:hidden_act}{{2.1.1}{6}{CBOW Model}{equation.2.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces The Skip-gram model}}{7}{figure.caption.7}}
\newlabel{fig:sg}{{2.2}{7}{The Skip-gram model}{figure.caption.7}{}}
\citation{w2v:parameter_learning}
\citation{w2v:mikolov_2013_distributed}
\citation{w2v:mikolov_2013_distributed}
\citation{w2v:mikolov_2013_distributed}
\citation{w2v:tensor_flow}
\citation{w2v:language_similarities}
\citation{w2v:mikolov_2013_distributed}
\citation{w2v:mikolov_2013_distributed}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Skip-gram Model}{8}{subsection.2.1.2}}
\newlabel{eqn:sg_prob}{{2.1.8}{8}{Skip-gram Model}{equation.2.1.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{Skip-gram with negative sampling}{8}{section*.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Properties of Word2Vec embeddings}{9}{subsection.2.1.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Word2vec semantic regularities}}{9}{figure.caption.9}}
\newlabel{fig:sem_rel}{{2.3}{9}{Word2vec semantic regularities}{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Word2Vec word clustering}}{9}{figure.caption.10}}
\newlabel{fig:w2v_clustering}{{2.4}{9}{Word2Vec word clustering}{figure.caption.10}{}}
\citation{w2v:language_similarities}
\citation{w2v:mikolov_2013_distributed}
\citation{esn:jaeger:2001}
\citation{esn:jaeger_tutorial}
\citation{esn:jaeger:2001}
\citation{esn:scholarpedia:2007}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Word2Vec language translation property}}{10}{figure.caption.11}}
\newlabel{fig:w2v_translation}{{2.5}{10}{Word2Vec language translation property}{figure.caption.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Echo State Network (ESN)}{10}{section.2.2}}
\citation{esn:jaeger:2001}
\citation{esn:practical_guide}
\citation{esn:practical_guide}
\citation{esn:jaeger_tutorial}
\citation{esn:practical_guide}
\citation{esn:practical_guide}
\citation{esn:practical_guide}
\citation{esn:jaeger_tutorial}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}ESN Architecture}{11}{subsection.2.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Architecture of classical ESN}}{11}{figure.caption.12}}
\newlabel{fig:esn_arch}{{2.6}{11}{Architecture of classical ESN}{figure.caption.12}{}}
\citation{esn:jaeger:2001}
\citation{esn:practical_guide}
\citation{esn:jaeger_tutorial}
\citation{esn:practical_guide}
\citation{esn:practical_guide}
\citation{esn:jaeger_tutorial}
\citation{esn:jaeger_tutorial}
\newlabel{eqn:res_scaling}{{2.2.1}{12}{ESN Architecture}{equation.2.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Training ESN}{12}{subsection.2.2.2}}
\newlabel{ssec:esn_training}{{2.2.2}{12}{Training ESN}{subsection.2.2.2}{}}
\citation{esn:practical_guide}
\citation{esn:optimization_leaky_neurons}
\newlabel{eqn:res_update}{{2.2.2}{13}{Training ESN}{equation.2.2.2}{}}
\newlabel{eqn:res_state}{{2.2.3}{13}{Training ESN}{equation.2.2.3}{}}
\newlabel{eqn:esn_output}{{2.2.4}{13}{Training ESN}{equation.2.2.4}{}}
\citation{xavier:2013:RT}
\citation{tra:xavier_hri}
\citation{tra:xavier_wermter:2014}
\citation{xavier:2013:RT}
\citation{tra:bates:1982}
\citation{gc:goldberg:1995}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Related Work and Open Issues}{15}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{issues}{{3}{15}{Related Work and Open Issues}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Overview of $\theta RARes$ Model}{15}{section.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Word2vec semantic regularities}}{16}{figure.caption.13}}
\newlabel{fig:tra_gc}{{3.1}{16}{Word2vec semantic regularities}{figure.caption.13}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Localist vector representation of sentence}}{16}{table.caption.14}}
\newlabel{tab:localist_representation}{{3.1}{16}{Localist vector representation of sentence}{table.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Word2vec semantic regularities}}{16}{figure.caption.15}}
\newlabel{fig:xavier_model}{{3.2}{16}{Word2vec semantic regularities}{figure.caption.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Limitation of $\theta RARes$ model }{17}{subsection.3.1.1}}
\@writefile{toc}{\contentsline {paragraph}{Example 1: Ambiguous Sentences}{17}{section*.16}}
\newlabel{eg-1:sent-1}{{1}{17}{Example 1: Ambiguous Sentences}{Item.1}{}}
\newlabel{eg-1:sent-2}{{2}{17}{Example 1: Ambiguous Sentences}{Item.2}{}}
\newlabel{eg-1:sent-3}{{3}{17}{Example 1: Ambiguous Sentences}{Item.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Training with Sentence \ref  {eg-1:sent-1}:}{17}{section*.17}}
\@writefile{toc}{\contentsline {paragraph}{Training with Sentence \ref  {eg-1:sent-2}:}{18}{section*.19}}
\@writefile{toc}{\contentsline {paragraph}{Testing with Sentence \ref  {eg-1:sent-3}:}{18}{section*.21}}
\newlabel{eg-1:sent-i}{{{{(i)}}}{18}{Testing with Sentence \ref {eg-1:sent-3}:}{Item.4}{}}
\newlabel{eg-1:sent-ii}{{{{(ii)}}}{18}{Testing with Sentence \ref {eg-1:sent-3}:}{Item.5}{}}
\newlabel{eg-1:sent-iii}{{{{(iii)}}}{18}{Testing with Sentence \ref {eg-1:sent-3}:}{Item.6}{}}
\citation{xavier:2013:RT}
\citation{tra:xavier_wermter:2014}
\citation{w2v:mikolov_2013_distributed}
\citation{w2v:regularities_in_word_representations}
\@writefile{toc}{\contentsline {paragraph}{Example 2: Ambiguous and Polysemous Sentences}{19}{section*.23}}
\newlabel{eg-2:sent-1}{{1}{19}{Example 2: Ambiguous and Polysemous Sentences}{Item.7}{}}
\newlabel{eg-2:sent-2}{{2}{19}{Example 2: Ambiguous and Polysemous Sentences}{Item.8}{}}
\@writefile{toc}{\contentsline {paragraph}{Training model on sentence \ref  {eg-2:sent-1}}{19}{section*.24}}
\@writefile{toc}{\contentsline {paragraph}{Testing model on sentence \ref  {eg-2:sent-2}}{19}{section*.25}}
\@writefile{toc}{\contentsline {paragraph}{Issue with Polysemous words:}{19}{section*.26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Research Hypothesis}{19}{subsection.3.1.2}}
\citation{xavier:2013:RT}
\citation{w2v:mikolov_2013_distributed}
\citation{esn:scholarpedia:2007}
\citation{esn:practical_guide}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Approaching Word2Vec-ESN Language Model}{21}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{approach}{{4}{21}{Approaching Word2Vec-ESN Language Model}{chapter.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Word2Vec-ESN Language Model}{21}{section.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Architecture of Word2Vec-ESN model}}{21}{figure.caption.27}}
\newlabel{fig:model_arch}{{4.1}{21}{Architecture of Word2Vec-ESN model}{figure.caption.27}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Model initialization}{21}{subsection.4.1.1}}
\citation{xavier:2013:RT}
\citation{xavier:2013:RT}
\citation{xavier:2013:RT}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Training model}{22}{subsection.4.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Neural comprehension of Word2Vec-ESN Model}}{22}{figure.caption.28}}
\newlabel{fig:model_variant_1}{{4.2}{22}{Neural comprehension of Word2Vec-ESN Model}{figure.caption.28}{}}
\citation{xavier:2013:RT}
\citation{xavier:2013:RT}
\citation{xavier:2013:RT}
\citation{xavier:2013:RT}
\newlabel{eg:SCL}{{1}{23}{Training model}{Item.9}{}}
\newlabel{eg:SFL}{{2}{23}{Training model}{Item.10}{}}
\@writefile{toc}{\contentsline {paragraph}{Decoding Output: }{23}{section*.29}}
\citation{xavier:2013:RT}
\citation{xavier:2013:RT}
\citation{xavier:2013:RT}
\citation{end-to-end}
\citation{xavier:2013:RT}
\citation{xavier:2013:RT}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Evaluation Metrics}{24}{subsection.4.1.3}}
\newlabel{sec:evaluation_metrics_1}{{4.1.3}{24}{Evaluation Metrics}{subsection.4.1.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Variant of Word2Vec-ESN Model }{24}{section.4.2}}
\newlabel{sec:model_variant}{{4.2}{24}{Variant of Word2Vec-ESN Model}{section.4.2}{}}
\newlabel{tab:argument-predicate}{{\caption@xref {tab:argument-predicate}{ on input line 64}}{25}{Variant of Word2Vec-ESN Model}{table.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Variant of Word2Vec-ESN language model}}{25}{figure.caption.31}}
\newlabel{fig:model_variant_2}{{4.3}{25}{Variant of Word2Vec-ESN language model}{figure.caption.31}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Training model variant}{25}{subsection.4.2.1}}
\citation{confusion_martrix:1998}
\citation{macro_average:2005}
\citation{conll:2004}
\citation{conll:2005}
\citation{classification_scores:2009}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Decoding Output}{26}{subsection.4.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Evaluation Metrics}{26}{subsection.4.2.3}}
\newlabel{sec:evaluation_metrics_2}{{4.2.3}{26}{Evaluation Metrics}{subsection.4.2.3}{}}
\citation{accuracy_paradox_1:2008}
\citation{accuracy_paradox_2:2014}
\citation{classification_scores:2009}
\citation{classification_scores:2009}
\citation{classification_scores:2009}
\newlabel{eqn:accuracy}{{4.2.1}{27}{Evaluation Metrics}{equation.4.2.1}{}}
\newlabel{eqn:precision}{{4.2.2}{27}{Evaluation Metrics}{equation.4.2.2}{}}
\newlabel{eqn:recall}{{4.2.3}{27}{Evaluation Metrics}{equation.4.2.3}{}}
\newlabel{eqn:precision}{{4.2.4}{27}{Evaluation Metrics}{equation.4.2.4}{}}
\citation{xavier:2013:RT}
\citation{xavier:2013:RT}
\citation{xavier:2013:RT}
\citation{xavier:2013:RT}
\citation{xavier:2013:RT}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Dataset and pre-processing}{28}{section.4.3}}
\newlabel{datasets}{{4.3}{28}{Dataset and pre-processing}{section.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Corpus For TRA Task}{28}{subsection.4.3.1}}
\citation{w2v:mikolov_2013_efficien}
\citation{w2v:mikolov_2013_distributed}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Different type of meaning realtions}}{29}{figure.caption.32}}
\newlabel{fig:meaning_realtions}{{4.4}{29}{Different type of meaning realtions}{figure.caption.32}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Corpus For Training Word2Vec Model}{29}{subsection.4.3.2}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Obtaining Word Embeddings}{29}{section.4.4}}
\newlabel{get_word_embeddings}{{4.4}{29}{Obtaining Word Embeddings}{section.4.4}{}}
\citation{w2v:gensim_api}
\citation{xavier:2013:RT}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Experiments and Results}{31}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{results}{{5}{31}{Experiments and Results}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Input and Ouput Coding}{31}{section.5.1}}
\@writefile{toc}{\contentsline {paragraph}{Word2Vec-ESN model:}{31}{section*.33}}
\@writefile{toc}{\contentsline {paragraph}{Word2Vec-ESN model variant:}{31}{section*.34}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Experiments}{32}{section.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Experiment-1: Learning thematic roles}{32}{subsection.5.2.1}}
\@writefile{toc}{\contentsline {paragraph}{Model Variant-1:}{32}{section*.35}}
\@writefile{toc}{\contentsline {paragraph}{Model Variant-2:}{32}{section*.36}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Experiment-2: Generalization Capabilities}{32}{subsection.5.2.2}}
\newlabel{exp-2}{{5.2.2}{32}{Experiment-2: Generalization Capabilities}{subsection.5.2.2}{}}
\citation{xavier:2013:RT}
\@writefile{toc}{\contentsline {paragraph}{Word2Vec-ESN:}{33}{section*.37}}
\@writefile{toc}{\contentsline {paragraph}{Word2Vec-ESN Variant:}{33}{section*.40}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Mean and standard deviation of meaning and sentence error on train and test set of coprus-462 in different learning modes.\relax }}{34}{table.caption.39}}
\newlabel{tab:corpus-462_errors}{{5.1}{34}{Mean and standard deviation of meaning and sentence error on train and test set of coprus-462 in different learning modes.\relax }{table.caption.39}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Training and testing classification scores for individual roles when using Word2Vec-ESN model variant.\relax }}{34}{table.caption.42}}
\newlabel{tab:classsification-scores-21}{{5.2}{34}{Training and testing classification scores for individual roles when using Word2Vec-ESN model variant.\relax }{table.caption.42}{}}
\citation{xavier:2013:RT}
\citation{xavier:2013:RT}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Normalized confusion matrix on corpus 462 with Word2Vec-ESN model variant}}{35}{figure.caption.43}}
\newlabel{fig:confusion_matrix}{{5.1}{35}{Normalized confusion matrix on corpus 462 with Word2Vec-ESN model variant}{figure.caption.43}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}Experiment-3: Effect of Corpus structure}{35}{subsection.5.2.3}}
\citation{xavier:2013:RT}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces Mean and standard deviation of meaning and sentence error on train and test set of coprus-462 in different learning modes.\relax }}{36}{table.caption.45}}
\newlabel{tab:corpus-structure}{{5.3}{36}{Mean and standard deviation of meaning and sentence error on train and test set of coprus-462 in different learning modes.\relax }{table.caption.45}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.4}Experiment-4: Effect of Reservoir size}{37}{subsection.5.2.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces \textbf  {Effect of reservoir size on cross validation errors on Model Variant-1:} Description goes here.\relax }}{37}{figure.caption.46}}
\newlabel{fig:reservoir_size_1}{{5.2}{37}{\textbf {Effect of reservoir size on cross validation errors on Model Variant-1:} Description goes here.\relax }{figure.caption.46}{}}
\citation{end-to-end}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces \textbf  {Effect of reservoir size on classification scores of Model Varinat-2:} Description goes here.\relax }}{38}{figure.caption.47}}
\newlabel{fig:reservoir_size_2}{{5.3}{38}{\textbf {Effect of reservoir size on classification scores of Model Varinat-2:} Description goes here.\relax }{figure.caption.47}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.5}Experiment-5: Effect of Corpus size}{38}{subsection.5.2.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces \textbf  {Effect of corpus size on cross validation errors:} Description goes here.\relax }}{39}{figure.caption.48}}
\newlabel{fig:corpus_size_1}{{5.4}{39}{\textbf {Effect of corpus size on cross validation errors:} Description goes here.\relax }{figure.caption.48}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces \textbf  {Effect of corpus size on cross validation errors using localist word vector as reported in [ref?]:} Description goes here.\relax }}{40}{figure.caption.49}}
\newlabel{fig:corpus_size_xavier}{{5.5}{40}{\textbf {Effect of corpus size on cross validation errors using localist word vector as reported in [ref?]:} Description goes here.\relax }{figure.caption.49}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.6}Experiment-6: Neural output activity of the model}{40}{subsection.5.2.6}}
\newlabel{eg-1:or-sent-17}{{1}{40}{Experiment-6: Neural output activity of the model}{Item.13}{}}
\newlabel{eg-1:or-sent-23}{{2}{40}{Experiment-6: Neural output activity of the model}{Item.14}{}}
\newlabel{eg-1:or-sent-27}{{3}{40}{Experiment-6: Neural output activity of the model}{Item.15}{}}
\newlabel{eg-1:or-sent-16}{{4}{40}{Experiment-6: Neural output activity of the model}{Item.16}{}}
\citation{tra:tra:xavier_hri}
\citation{tra:tra:xavier_hri}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces \textbf  {Effect of corpus size on cross validation errors using localist word vector as reported in [ref?]:} Description goes here.\relax }}{41}{figure.caption.50}}
\newlabel{fig:act_analysis_1}{{5.6}{41}{\textbf {Effect of corpus size on cross validation errors using localist word vector as reported in [ref?]:} Description goes here.\relax }{figure.caption.50}{}}
\newlabel{eg-1:or-sent-17}{{1}{41}{Experiment-6: Neural output activity of the model}{Item.17}{}}
\newlabel{eg-1:or-sent-23}{{2}{41}{Experiment-6: Neural output activity of the model}{Item.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces \textbf  {Effect of corpus size on cross validation errors using localist word vector as reported in [ref?]:} Description goes here.\relax }}{41}{figure.caption.51}}
\newlabel{fig:act_analysis_2}{{5.7}{41}{\textbf {Effect of corpus size on cross validation errors using localist word vector as reported in [ref?]:} Description goes here.\relax }{figure.caption.51}{}}
\citation{tra:tra:xavier_hri}
\citation{tra:xavier_hri}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces \textbf  {Effect of corpus size on cross validation errors using localist word vector as reported in [ref?]:} Description goes here.\relax }}{42}{figure.caption.52}}
\newlabel{fig:act_analysis_3}{{5.8}{42}{\textbf {Effect of corpus size on cross validation errors using localist word vector as reported in [ref?]:} Description goes here.\relax }{figure.caption.52}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.7}Experiment-7: Generalization on new corpus}{42}{subsection.5.2.7}}
\@writefile{lot}{\contentsline {table}{\numberline {5.4}{\ignorespaces Generalization error in sentence continuous learning mode for corpus-373.\relax }}{43}{table.caption.54}}
\newlabel{tab:corpus_373}{{5.4}{43}{Generalization error in sentence continuous learning mode for corpus-373.\relax }{table.caption.54}{}}
\citation{crf:intro:sutton}
\citation{end-to-end}
\citation{esn:esn_crf}
\citation{esn:esn_crf}
\citation{w2v:regularities_in_word_representations}
\citation{w2v:mikolov_2013_efficient}
\citation{w2v:language_similarities}
\citation{hinaut_multiple_lang}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusion And Future Work}{45}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{conclusion}{{6}{45}{Conclusion And Future Work}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Conclusion}{45}{section.6.1}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Future Work}{45}{section.6.2}}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Nomenclature}{47}{appendix.A}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{app:nomenclature}{{A}{47}{Nomenclature}{appendix.A}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}Additional Proofs}{49}{appendix.B}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{app:proofs}{{B}{49}{Additional Proofs}{appendix.B}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {C}Complete Simulation Results}{51}{appendix.C}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{app:completeResults}{{C}{51}{Complete Simulation Results}{appendix.C}{}}
\bibstyle{plain}
\bibdata{thesis,esn,word2vec}
\bibcite{w2v:tensor_flow}{1}
\bibcite{confusion_martrix:1998}{2}
\bibcite{w2v:baroni:2014}{3}
\bibcite{srl:info_extraction:hri}{4}
\bibcite{tra:bates:1982}{5}
\bibcite{rnn:gradiant_problem:bengio}{6}
\bibcite{conll:2005}{7}
\bibcite{conll:2004}{8}
\bibcite{charniak_parser:2000}{9}
\bibcite{esn:esn_crf}{10}
\bibcite{srl:collobert:2011}{11}
\bibcite{rnn:elman:1990}{12}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{53}{appendix.C}}
\bibcite{gc:goldberg:1995}{13}
\bibcite{xavier:2013:RT}{14}
\bibcite{tra:xavier_hri}{15}
\bibcite{hinaut_multiple_lang}{16}
\bibcite{tra:xavier_wermter:2014}{17}
\bibcite{esn:scholarpedia:2007}{18}
\bibcite{esn:jaeger_tutorial}{19}
\bibcite{esn:jaeger:2001}{20}
\bibcite{esn:NIPS:2003}{21}
\bibcite{esn:optimization_leaky_neurons}{22}
\bibcite{Koomen:2005}{23}
\bibcite{srl:machine_translation}{24}
\bibcite{esn:practical_guide}{25}
\bibcite{w2v:mikolov_2013_distributed}{26}
\bibcite{w2v:mikolov_2013_efficient}{27}
\bibcite{w2v:language_similarities}{28}
\bibcite{macro_average:2005}{29}
\bibcite{srl:text_categorization:persson}{30}
\bibcite{pradhan:2005}{31}
\bibcite{srl:pradhan:2004}{32}
\bibcite{w2v:gensim_api}{33}
\bibcite{w2v:parameter_learning}{34}
\bibcite{classification_scores:2009}{35}
\bibcite{crf:intro:sutton}{36}
\bibcite{accuracy_paradox_1:2008}{37}
\bibcite{w2v:regularities_in_word_representations}{38}
\bibcite{esn:learn_gs}{39}
\bibcite{accuracy_paradox_2:2014}{40}
\bibcite{srl:sentiment:wang}{41}
\bibcite{end-to-end}{42}
\newlabel{sec:urheber}{{C}{57}{Erkl\"arung der Urheberschaft}{appendix*.56}{}}
\newlabel{sec:urheber}{{C}{59}{Erkl\"arung zur Ver\"offentlichung}{appendix*.57}{}}
